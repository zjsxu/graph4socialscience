"""
æ–‡æœ¬å¤„ç†å™¨æ¨¡å—

å®žçŽ°æ–‡æœ¬é¢„å¤„ç†ã€è¯­è¨€æ£€æµ‹ã€åˆ†è¯å’Œè§„èŒƒåŒ–åŠŸèƒ½ã€‚
æ ¹æ®éœ€æ±‚5.3å’Œ3.4ï¼Œæ”¯æŒè‹±æ–‡å’Œä¸­æ–‡çš„æ–‡æœ¬å¤„ç†æµç¨‹ã€‚
"""

import re
import string
from typing import Dict, List, Any, Optional
import logging
from tqdm import tqdm
from dataclasses import dataclass

# å¯¼å…¥NLTKå’Œjieba
try:
    import nltk
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.corpus import stopwords
    from nltk.stem import PorterStemmer
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    logging.warning("NLTK not available, falling back to basic tokenization")

try:
    import jieba
    import jieba.posseg as pseg
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    logging.warning("jieba not available, Chinese processing will be limited")

from ..core.data_models import TOCDocument, ProcessedDocument, Window
from ..core.config import Config
from ..core.error_handler import ErrorHandler


@dataclass
class LanguageDetectionResult:
    """è¯­è¨€æ£€æµ‹ç»“æžœ"""
    language: str
    confidence: float
    detected_features: Dict[str, Any]


class LanguageDetector:
    """
    è¯­è¨€æ£€æµ‹å™¨
    
    ä½¿ç”¨ç®€å•çš„å¯å‘å¼è§„åˆ™æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼ˆè‹±æ–‡/ä¸­æ–‡ï¼‰ã€‚
    """
    
    def __init__(self):
        # ä¸­æ–‡å­—ç¬¦èŒƒå›´
        self.chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
        # è‹±æ–‡å­—ç¬¦èŒƒå›´
        self.english_pattern = re.compile(r'[a-zA-Z]')
    
    def detect_language(self, text: str) -> LanguageDetectionResult:
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            LanguageDetectionResult: æ£€æµ‹ç»“æžœ
        """
        if not text or not text.strip():
            return LanguageDetectionResult(
                language='unknown',
                confidence=0.0,
                detected_features={}
            )
        
        # ç»Ÿè®¡ä¸­æ–‡å’Œè‹±æ–‡å­—ç¬¦æ•°é‡
        chinese_chars = len(self.chinese_pattern.findall(text))
        english_chars = len(self.english_pattern.findall(text))
        total_chars = len(text.strip())
        
        # è®¡ç®—æ¯”ä¾‹
        chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
        english_ratio = english_chars / total_chars if total_chars > 0 else 0
        
        # æ£€æµ‹ç‰¹å¾
        features = {
            'chinese_chars': chinese_chars,
            'english_chars': english_chars,
            'total_chars': total_chars,
            'chinese_ratio': chinese_ratio,
            'english_ratio': english_ratio
        }
        
        # è¯­è¨€åˆ¤æ–­é€»è¾‘
        if chinese_ratio > 0.1:  # å¦‚æžœä¸­æ–‡å­—ç¬¦è¶…è¿‡10%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡
            language = 'chinese'
            confidence = min(chinese_ratio * 2, 1.0)
        elif english_ratio > 0.3:  # å¦‚æžœè‹±æ–‡å­—ç¬¦è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯è‹±æ–‡
            language = 'english'
            confidence = min(english_ratio * 1.5, 1.0)
        else:
            # é»˜è®¤åˆ¤æ–­ä¸ºè‹±æ–‡
            language = 'english'
            confidence = 0.5
        
        return LanguageDetectionResult(
            language=language,
            confidence=confidence,
            detected_features=features
        )


class EnglishTokenizer:
    """
    è‹±æ–‡åˆ†è¯å™¨
    
    ä½¿ç”¨NLTKè¿›è¡Œè‹±æ–‡æ–‡æœ¬çš„åˆ†è¯å¤„ç†ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.use_stemming = config.get('use_stemming', False)
        self.remove_stopwords = config.get('remove_stopwords', False)
        
        # åˆå§‹åŒ–NLTKç»„ä»¶
        if NLTK_AVAILABLE:
            try:
                # ç¡®ä¿å¿…è¦çš„NLTKæ•°æ®å·²ä¸‹è½½
                nltk.data.find('tokenizers/punkt')
            except LookupError:
                logging.info("Downloading NLTK punkt tokenizer...")
                nltk.download('punkt', quiet=True)
            
            try:
                nltk.data.find('corpora/stopwords')
            except LookupError:
                logging.info("Downloading NLTK stopwords...")
                nltk.download('stopwords', quiet=True)
            
            self.stemmer = PorterStemmer() if self.use_stemming else None
            self.stop_words = set(stopwords.words('english')) if self.remove_stopwords else set()
        else:
            self.stemmer = None
            self.stop_words = set()
    
    def tokenize(self, text: str) -> List[str]:
        """
        è‹±æ–‡åˆ†è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†è¯ç»“æžœ
        """
        if not text or not text.strip():
            return []
        
        if NLTK_AVAILABLE:
            try:
                # ä½¿ç”¨NLTKåˆ†è¯
                tokens = word_tokenize(text.lower())
            except Exception as e:
                logging.warning(f"NLTK tokenization failed: {e}, falling back to simple split")
                tokens = self._simple_tokenize(text)
        else:
            tokens = self._simple_tokenize(text)
        
        # è¿‡æ»¤å’Œå¤„ç†
        processed_tokens = []
        for token in tokens:
            # è·³è¿‡æ ‡ç‚¹ç¬¦å·å’Œç©ºç™½
            if token in string.punctuation or not token.strip():
                continue
            
            # è·³è¿‡åœè¯
            if self.remove_stopwords and token.lower() in self.stop_words:
                continue
            
            # è¯å¹²æå–
            if self.stemmer:
                token = self.stemmer.stem(token)
            
            processed_tokens.append(token)
        
        return processed_tokens
    
    def _simple_tokenize(self, text: str) -> List[str]:
        """
        ç®€å•åˆ†è¯ï¼ˆåŽå¤‡æ–¹æ¡ˆï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†è¯ç»“æžœ
        """
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶æŒ‰ç©ºæ ¼åˆ†å‰²
        text = re.sub(r'[^\w\s]', ' ', text)
        return [token.strip().lower() for token in text.split() if token.strip()]


class ChineseTokenizer:
    """
    ä¸­æ–‡åˆ†è¯å™¨
    
    ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡æ–‡æœ¬çš„åˆ†è¯å¤„ç†ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.use_pos_tagging = config.get('use_pos_tagging', False)
        self.remove_stopwords = config.get('remove_stopwords', False)
        
        # ä¸­æ–‡å­—ç¬¦æ¨¡å¼
        self.chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
        
        # åˆå§‹åŒ–jieba
        if JIEBA_AVAILABLE:
            # è®¾ç½®jiebaæ—¥å¿—çº§åˆ«
            jieba.setLogLevel(logging.WARNING)
            
            # åŠ è½½ç”¨æˆ·è¯å…¸ï¼ˆå¦‚æžœæœ‰ï¼‰
            user_dict_path = config.get('user_dict_path')
            if user_dict_path:
                try:
                    jieba.load_userdict(user_dict_path)
                    logging.info(f"Loaded user dictionary: {user_dict_path}")
                except Exception as e:
                    logging.warning(f"Failed to load user dictionary: {e}")
        
        # ä¸­æ–‡åœè¯ï¼ˆåŸºç¡€é›†åˆï¼‰
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'åŽ»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'é‡Œ', 'å°±æ˜¯', 'è¿˜', 'æŠŠ', 'æ¯”', 'æˆ–è€…', 'ç­‰', 'ä½†æ˜¯', 'å¦‚æžœ'
        }
    
    def tokenize(self, text: str) -> List[str]:
        """
        ä¸­æ–‡åˆ†è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†è¯ç»“æžœ
        """
        if not text or not text.strip():
            return []
        
        if JIEBA_AVAILABLE:
            try:
                if self.use_pos_tagging:
                    # ä½¿ç”¨è¯æ€§æ ‡æ³¨
                    words = pseg.cut(text)
                    tokens = [word for word, pos in words if self._is_valid_pos(pos)]
                else:
                    # æ™®é€šåˆ†è¯
                    tokens = list(jieba.cut(text))
            except Exception as e:
                logging.warning(f"jieba tokenization failed: {e}, falling back to simple split")
                tokens = self._simple_tokenize(text)
        else:
            tokens = self._simple_tokenize(text)
        
        # è¿‡æ»¤å’Œå¤„ç†
        processed_tokens = []
        for token in tokens:
            token = token.strip()
            
            # è·³è¿‡ç©ºç™½å’Œçº¯æ ‡ç‚¹ç¬¦å·
            if not token or token in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘':
                continue
            
            # è·³è¿‡å•å­—ç¬¦ï¼ˆé™¤éžæ˜¯ä¸­æ–‡å­—ç¬¦ï¼‰
            if len(token) == 1 and not self.chinese_pattern.match(token):
                continue
            
            # è·³è¿‡çº¯æ•°å­—
            if token.isdigit():
                continue
            
            # è·³è¿‡åœè¯
            if self.remove_stopwords and token in self.stop_words:
                continue
            
            processed_tokens.append(token)
        
        return processed_tokens
    
    def _is_valid_pos(self, pos: str) -> bool:
        """
        æ£€æŸ¥è¯æ€§æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            pos: è¯æ€§æ ‡ç­¾
            
        Returns:
            bool: æ˜¯å¦æœ‰æ•ˆ
        """
        # ä¿ç•™åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰å®žè¯
        valid_pos = {'n', 'v', 'a', 'nr', 'ns', 'nt', 'nz', 'vn', 'an'}
        return pos and any(pos.startswith(p) for p in valid_pos)
    
    def _simple_tokenize(self, text: str) -> List[str]:
        """
        ç®€å•ä¸­æ–‡åˆ†è¯ï¼ˆåŽå¤‡æ–¹æ¡ˆï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†è¯ç»“æžœ
        """
        # æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²ï¼Œç„¶åŽæŒ‰å­—ç¬¦åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›ï¼Œã€]', text)
        tokens = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence:
                # ç®€å•æŒ‰å­—ç¬¦åˆ†å‰²ï¼ˆä¸ç†æƒ³ï¼Œä½†ä½œä¸ºåŽå¤‡ï¼‰
                chars = list(sentence)
                tokens.extend([char for char in chars if char.strip() and not char.isspace()])
        
        return tokens


class TextProcessor:
    """
    æ–‡æœ¬å¤„ç†å™¨ä¸»ç±»
    
    æ ¹æ®éœ€æ±‚5.3å’Œ3.4å®žçŽ°æ–‡æœ¬é¢„å¤„ç†ã€è¯­è¨€æ£€æµ‹ã€åˆ†è¯å’Œè§„èŒƒåŒ–åŠŸèƒ½ã€‚
    """
    
    def __init__(self, config: Optional[Config] = None):
        """
        åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config or Config()
        self.error_handler = ErrorHandler(self.config.get_section('error_handling'))
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.language_detector = LanguageDetector()
        
        # èŽ·å–æ–‡æœ¬å¤„ç†é…ç½®
        text_config = self.config.get_section('text_processing')
        
        self.english_tokenizer = EnglishTokenizer(text_config)
        self.chinese_tokenizer = ChineseTokenizer(text_config)
        
        # å¤„ç†é€‰é¡¹
        self.normalize_text = text_config.get('normalize_text', True)
        self.remove_punctuation = text_config.get('remove_punctuation', True)
        self.convert_to_lowercase = text_config.get('convert_to_lowercase', True)
        
        logging.info("TextProcessor initialized successfully")
    
    def process_document(self, doc: TOCDocument) -> ProcessedDocument:
        """
        å¤„ç†å•ä¸ªTOCæ–‡æ¡£
        
        Args:
            doc: TOCæ–‡æ¡£
            
        Returns:
            ProcessedDocument: å¤„ç†åŽçš„æ–‡æ¡£
        """
        try:
            # æ£€æµ‹è¯­è¨€
            if not doc.language:
                detection_result = self.detect_language(doc.text)
                doc.language = detection_result.language
                logging.debug(f"Detected language for document {doc.segment_id}: {doc.language}")
            
            # æ–‡æœ¬è§„èŒƒåŒ–
            cleaned_text = self.normalize_text_content(doc.text, doc.language)
            
            # åˆ†è¯
            tokens = self.tokenize(cleaned_text, doc.language)
            
            # åˆ›å»ºçª—å£ï¼ˆæ¯ä¸ªsegmentä½œä¸ºä¸€ä¸ªçª—å£ï¼‰
            window = Window(
                window_id=f"{doc.segment_id}_window",
                phrases=tokens,  # è¿™é‡Œå…ˆç”¨tokensï¼ŒåŽç»­ä¼šè¢«è¯ç»„æ›¿æ¢
                source_doc=doc.segment_id,
                state=doc.state or 'unknown',
                segment_id=doc.segment_id
            )
            
            # åˆ›å»ºå¤„ç†åŽçš„æ–‡æ¡£
            processed_doc = ProcessedDocument(
                original_doc=doc,
                cleaned_text=cleaned_text,
                tokens=tokens,
                phrases=[],  # è¯ç»„æŠ½å–åœ¨åŽç»­æ­¥éª¤è¿›è¡Œ
                windows=[window],
                processing_metadata={
                    'language': doc.language,
                    'token_count': len(tokens),
                    'original_text_length': len(doc.text),
                    'cleaned_text_length': len(cleaned_text)
                }
            )
            
            logging.debug(f"Processed document {doc.segment_id}: {len(tokens)} tokens")
            return processed_doc
            
        except Exception as e:
            error_msg = f"Failed to process document {doc.segment_id}: {e}"
            logging.error(error_msg)
            return self.error_handler.handle_processing_error(e, f"process_document:{doc.segment_id}")
    
    def detect_language(self, text: str) -> LanguageDetectionResult:
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            LanguageDetectionResult: æ£€æµ‹ç»“æžœ
        """
        try:
            return self.language_detector.detect_language(text)
        except Exception as e:
            logging.warning(f"Language detection failed: {e}, defaulting to English")
            return LanguageDetectionResult(
                language='english',
                confidence=0.5,
                detected_features={}
            )
    
    def normalize_text_content(self, text: str, language: str) -> str:
        """
        æ–‡æœ¬è§„èŒƒåŒ–å¤„ç†
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            language: æ–‡æœ¬è¯­è¨€
            
        Returns:
            str: è§„èŒƒåŒ–åŽçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        try:
            # åŸºç¡€æ¸…ç†
            normalized = text.strip()
            
            # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
            normalized = re.sub(r'\s+', ' ', normalized)
            
            # æ ¹æ®è¯­è¨€è¿›è¡Œç‰¹å®šå¤„ç†
            if language == 'english':
                normalized = self._normalize_english_text(normalized)
            elif language == 'chinese':
                normalized = self._normalize_chinese_text(normalized)
            
            return normalized
            
        except Exception as e:
            logging.warning(f"Text normalization failed: {e}, returning original text")
            return text
    
    def tokenize(self, text: str, language: str) -> List[str]:
        """
        åˆ†è¯å¤„ç†
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            language: æ–‡æœ¬è¯­è¨€
            
        Returns:
            List[str]: åˆ†è¯ç»“æžœ
        """
        if not text:
            return []
        
        try:
            if language == 'english':
                return self.english_tokenizer.tokenize(text)
            elif language == 'chinese':
                return self.chinese_tokenizer.tokenize(text)
            else:
                # é»˜è®¤ä½¿ç”¨è‹±æ–‡åˆ†è¯
                logging.warning(f"Unknown language {language}, using English tokenizer")
                return self.english_tokenizer.tokenize(text)
                
        except Exception as e:
            logging.error(f"Tokenization failed: {e}")
            # åŽå¤‡æ–¹æ¡ˆï¼šç®€å•åˆ†å‰²
            return text.split()
    
    def _normalize_english_text(self, text: str) -> str:
        """
        è‹±æ–‡æ–‡æœ¬è§„èŒƒåŒ–
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            str: è§„èŒƒåŒ–åŽçš„æ–‡æœ¬
        """
        # è½¬æ¢ä¸ºå°å†™
        if self.convert_to_lowercase:
            text = text.lower()
        
        # ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        if self.remove_punctuation:
            # ä¿ç•™å¥å·ã€é€—å·ç­‰åŸºæœ¬æ ‡ç‚¹ï¼Œç§»é™¤å…¶ä»–ç‰¹æ®Šå­—ç¬¦
            text = re.sub(r'[^\w\s.,;:!?-]', ' ', text)
        
        # å¤„ç†ç¼©å†™
        text = re.sub(r"'s\b", ' is', text)
        text = re.sub(r"'re\b", ' are', text)
        text = re.sub(r"'ve\b", ' have', text)
        text = re.sub(r"'ll\b", ' will', text)
        text = re.sub(r"'d\b", ' would', text)
        text = re.sub(r"n't\b", ' not', text)
        
        # è§„èŒƒåŒ–ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def _normalize_chinese_text(self, text: str) -> str:
        """
        ä¸­æ–‡æ–‡æœ¬è§„èŒƒåŒ–
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            str: è§„èŒƒåŒ–åŽçš„æ–‡æœ¬
        """
        # ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        if self.remove_punctuation:
            # ä¿ç•™ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
            text = re.sub(r'[^\u4e00-\u9fff\w\sã€‚ï¼Œã€ï¼›ï¼šï¼ï¼Ÿï¼ˆï¼‰ã€ã€‘""''â€”â€¦]', ' ', text)
        
        # ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        text = text.replace('ï¼Œ', ',')
        text = text.replace('ã€‚', '.')
        text = text.replace('ï¼', '!')
        text = text.replace('ï¼Ÿ', '?')
        text = text.replace('ï¼›', ';')
        text = text.replace('ï¼š', ':')
        
        # è§„èŒƒåŒ–ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def batch_process_documents(self, documents: List[TOCDocument]) -> List[ProcessedDocument]:
        """
        æ‰¹é‡å¤„ç†æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            List[ProcessedDocument]: å¤„ç†åŽçš„æ–‡æ¡£åˆ—è¡¨
        """
        processed_docs = []
        
        for i, doc in enumerate(documents):
            try:
                processed_doc = self.process_document(doc)
                processed_docs.append(processed_doc)
                
                if (i + 1) % 100 == 0:
                    logging.info(f"Processed {i + 1}/{len(documents)} documents")
                    
            except Exception as e:
                logging.error(f"Failed to process document {doc.segment_id}: {e}")
                # ç»§ç»­å¤„ç†å…¶ä»–æ–‡æ¡£
                continue
        
        logging.info(f"Batch processing completed: {len(processed_docs)}/{len(documents)} documents processed")
        return processed_docs
    
    def get_processing_statistics(self, processed_docs: List[ProcessedDocument]) -> Dict[str, Any]:
        """
        èŽ·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            processed_docs: å¤„ç†åŽçš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        if not processed_docs:
            return {}
        
        # è¯­è¨€åˆ†å¸ƒ
        language_counts = {}
        total_tokens = 0
        total_original_length = 0
        total_cleaned_length = 0
        
        for doc in tqdm(processed_docs, desc="ðŸ“„ Processing documents", unit="doc"):
            lang = doc.processing_metadata.get('language', 'unknown')
            language_counts[lang] = language_counts.get(lang, 0) + 1
            
            total_tokens += doc.processing_metadata.get('token_count', 0)
            total_original_length += doc.processing_metadata.get('original_text_length', 0)
            total_cleaned_length += doc.processing_metadata.get('cleaned_text_length', 0)
        
        return {
            'total_documents': len(processed_docs),
            'language_distribution': language_counts,
            'total_tokens': total_tokens,
            'average_tokens_per_doc': total_tokens / len(processed_docs),
            'total_original_length': total_original_length,
            'total_cleaned_length': total_cleaned_length,
            'text_reduction_ratio': 1 - (total_cleaned_length / total_original_length) if total_original_length > 0 else 0
        }