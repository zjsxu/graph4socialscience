"""
åŠ¨æ€åœè¯å‘ç°å™¨

å®ç°åŸºäºTF-IDFçš„åŠ¨æ€åœè¯å‘ç°æœºåˆ¶ï¼Œç”¨äºè¯†åˆ«é«˜é¢‘ä½åŒºåˆ†åº¦çš„è¯ç»„ã€‚
æ ¹æ®éœ€æ±‚4.1-4.5ï¼Œæä¾›å¯è§£é‡Šçš„åœè¯å‘ç°å’Œåˆå¹¶åŠŸèƒ½ã€‚
"""

import logging
from tqdm import tqdm
import math
from collections import Counter, defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Set, Any, Optional, Tuple
import json

from ..core.data_models import ProcessedDocument, Phrase


@dataclass
class TFIDFScore:
    """TF-IDFåˆ†æ•°æ•°æ®ç»“æ„"""
    phrase: str
    tf: float  # è¯é¢‘
    df: int    # æ–‡æ¡£é¢‘ç‡
    idf: float # é€†æ–‡æ¡£é¢‘ç‡
    tfidf: float # TF-IDFåˆ†æ•°
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'phrase': self.phrase,
            'tf': self.tf,
            'df': self.df,
            'idf': self.idf,
            'tfidf': self.tfidf
        }


@dataclass
class StopwordDiscoveryResult:
    """åœè¯å‘ç°ç»“æœ"""
    dynamic_stopwords: Set[str]
    merged_stopwords: Set[str]
    tfidf_scores: Dict[str, TFIDFScore]
    discovery_metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'dynamic_stopwords': list(self.dynamic_stopwords),
            'merged_stopwords': list(self.merged_stopwords),
            'tfidf_scores': {phrase: score.to_dict() for phrase, score in self.tfidf_scores.items()},
            'discovery_metadata': self.discovery_metadata
        }


class DynamicStopwordDiscoverer:
    """
    åŠ¨æ€åœè¯å‘ç°å™¨
    
    æ ¹æ®éœ€æ±‚4.1-4.5å®ç°ï¼š
    - 4.1: è®¡ç®—å…¨è¯­æ–™çš„TF-IDFæŒ‡æ ‡
    - 4.2: å‘ç°è·¨æ–‡æ¡£æ™®éé«˜é¢‘ä½†åŒºåˆ†åº¦æä½çš„è¯ç»„
    - 4.4: å°†åŠ¨æ€åœè¯è¡¨ä¸äººå·¥åœè¯è¡¨åˆå¹¶
    - 4.5: åº”ç”¨åˆå¹¶åçš„åœè¯è¡¨ä½œä¸ºå¼ºçº¦æŸ
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–åŠ¨æ€åœè¯å‘ç°å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«é˜ˆå€¼å’Œè·¯å¾„è®¾ç½®
        """
        self.config = config
        self.tfidf_threshold = config.get('tfidf_threshold', 0.1)
        self.frequency_threshold = config.get('frequency_threshold', 0.8)
        self.min_document_frequency = config.get('min_document_frequency', 2)
        self.static_stopwords_path = config.get('static_stopwords_path', 'data/stopwords.txt')
        self.enable_dynamic_discovery = config.get('enable_dynamic_discovery', True)
        
        # æ—¥å¿—è®°å½•
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½é™æ€åœè¯è¡¨
        self.static_stopwords = self._load_static_stopwords()
        
    def _load_static_stopwords(self) -> Set[str]:
        """
        åŠ è½½é™æ€åœè¯è¡¨
        
        Returns:
            é™æ€åœè¯é›†åˆ
        """
        static_stopwords = set()
        
        if self.static_stopwords_path:
            stopwords_path = Path(self.static_stopwords_path)
            
            if stopwords_path.exists():
                try:
                    with open(stopwords_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            word = line.strip()
                            if word and not word.startswith('#'):
                                static_stopwords.add(word)
                    
                    self.logger.info(f"Loaded {len(static_stopwords)} static stopwords from {stopwords_path}")
                    
                except Exception as e:
                    self.logger.warning(f"Failed to load static stopwords from {stopwords_path}: {e}")
            else:
                self.logger.info(f"Static stopwords file not found: {stopwords_path}")
        
        # æ·»åŠ åŸºç¡€åœè¯
        basic_stopwords = {
            # è‹±æ–‡åŸºç¡€åœè¯
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall',
            'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
            'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their',
            # ä¸­æ–‡åŸºç¡€åœè¯
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'éƒ½', 'ä¸€', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä¼š', 'ç€',
            'ä¸ª', 'ä¸Š', 'æ¥', 'ä¸‹', 'å¯¹', 'ä»', 'æŠŠ', 'è¢«', 'è®©', 'ä½¿', 'ç»™', 'ä¸º', 'ä¸', 'åŠ', 'æˆ–', 'ä½†', 'è€Œ', 'ä¸”',
            'è¿™', 'é‚£', 'äº›', 'æ­¤', 'å…¶', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘', 'ä½ ', 'æ‚¨', 'ä»¬', 'è‡ªå·±', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ'
        }
        
        static_stopwords.update(basic_stopwords)
        
        return static_stopwords
    
    def discover_stopwords(self, processed_docs: List[ProcessedDocument]) -> StopwordDiscoveryResult:
        """
        å‘ç°åŠ¨æ€åœè¯
        
        Args:
            processed_docs: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            åœè¯å‘ç°ç»“æœ
        """
        if not self.enable_dynamic_discovery:
            self.logger.info("Dynamic stopword discovery is disabled")
            return StopwordDiscoveryResult(
                dynamic_stopwords=set(),
                merged_stopwords=self.static_stopwords.copy(),
                tfidf_scores={},
                discovery_metadata={'enabled': False}
            )
        
        # æå–è¯ç»„è¯­æ–™åº“
        phrase_corpus = []
        for doc in tqdm(processed_docs, desc="ğŸ“„ Processing documents", unit="doc"):
            if doc.phrases:
                phrase_corpus.append(doc.phrases)
        
        if not phrase_corpus:
            self.logger.warning("No phrases found in processed documents")
            return StopwordDiscoveryResult(
                dynamic_stopwords=set(),
                merged_stopwords=self.static_stopwords.copy(),
                tfidf_scores={},
                discovery_metadata={'error': 'no_phrases'}
            )
        
        # è®¡ç®—TF-IDFåˆ†æ•°
        tfidf_scores = self.calculate_tfidf_matrix(phrase_corpus)
        
        # è¯†åˆ«ä½åŒºåˆ†åº¦è¯ç»„
        dynamic_stopwords = self.identify_low_discrimination_phrases(tfidf_scores)
        
        # åˆå¹¶åœè¯è¡¨
        merged_stopwords = self.merge_stopword_lists(dynamic_stopwords)
        
        # ç”Ÿæˆå‘ç°å…ƒæ•°æ®
        discovery_metadata = {
            'total_documents': len(phrase_corpus),
            'total_unique_phrases': len(tfidf_scores),
            'dynamic_stopwords_count': len(dynamic_stopwords),
            'static_stopwords_count': len(self.static_stopwords),
            'merged_stopwords_count': len(merged_stopwords),
            'tfidf_threshold': self.tfidf_threshold,
            'frequency_threshold': self.frequency_threshold,
            'min_document_frequency': self.min_document_frequency
        }
        
        self.logger.info(f"Discovered {len(dynamic_stopwords)} dynamic stopwords")
        self.logger.info(f"Total merged stopwords: {len(merged_stopwords)}")
        
        return StopwordDiscoveryResult(
            dynamic_stopwords=dynamic_stopwords,
            merged_stopwords=merged_stopwords,
            tfidf_scores=tfidf_scores,
            discovery_metadata=discovery_metadata
        )
    
    def calculate_tfidf_matrix(self, phrase_corpus: List[List[str]]) -> Dict[str, TFIDFScore]:
        """
        è®¡ç®—TF-IDFçŸ©é˜µ
        
        æ ¹æ®éœ€æ±‚4.1ï¼Œè®¡ç®—å…¨è¯­æ–™çš„TF-IDFæŒ‡æ ‡ã€‚
        
        Args:
            phrase_corpus: è¯ç»„è¯­æ–™åº“ï¼Œæ¯ä¸ªæ–‡æ¡£æ˜¯ä¸€ä¸ªè¯ç»„åˆ—è¡¨
            
        Returns:
            è¯ç»„åˆ°TF-IDFåˆ†æ•°çš„æ˜ å°„
        """
        if not phrase_corpus:
            return {}
        
        # ç»Ÿè®¡æ–‡æ¡£é¢‘ç‡ (DF)
        document_frequency = Counter()
        total_documents = len(phrase_corpus)
        
        # ç»Ÿè®¡æ¯ä¸ªè¯ç»„åœ¨å¤šå°‘ä¸ªæ–‡æ¡£ä¸­å‡ºç°
        for doc_phrases in phrase_corpus:
            unique_phrases = set(doc_phrases)
            for phrase in unique_phrases:
                document_frequency[phrase] += 1
        
        # è¿‡æ»¤ä½é¢‘è¯ç»„
        filtered_phrases = {
            phrase: df for phrase, df in document_frequency.items()
            if df >= self.min_document_frequency
        }
        
        # è®¡ç®—TF-IDFåˆ†æ•°
        tfidf_scores = {}
        
        for phrase, df in filtered_phrases.items():
            # è®¡ç®—å¹³å‡è¯é¢‘ (TF)
            total_tf = 0
            for doc_phrases in phrase_corpus:
                phrase_count = doc_phrases.count(phrase)
                if phrase_count > 0:
                    doc_length = len(doc_phrases)
                    if doc_length > 0:
                        total_tf += phrase_count / doc_length
            
            avg_tf = total_tf / total_documents if total_documents > 0 else 0
            
            # è®¡ç®—é€†æ–‡æ¡£é¢‘ç‡ (IDF)
            idf = math.log(total_documents / df) if df > 0 else 0
            
            # è®¡ç®—TF-IDF
            tfidf = avg_tf * idf
            
            tfidf_scores[phrase] = TFIDFScore(
                phrase=phrase,
                tf=avg_tf,
                df=df,
                idf=idf,
                tfidf=tfidf
            )
        
        self.logger.info(f"Calculated TF-IDF for {len(tfidf_scores)} phrases")
        
        return tfidf_scores
    
    def identify_low_discrimination_phrases(self, tfidf_scores: Dict[str, TFIDFScore]) -> Set[str]:
        """
        è¯†åˆ«ä½åŒºåˆ†åº¦è¯ç»„
        
        æ ¹æ®éœ€æ±‚4.2ï¼Œå‘ç°è·¨æ–‡æ¡£æ™®éé«˜é¢‘ä½†åŒºåˆ†åº¦æä½çš„è¯ç»„ã€‚
        
        Args:
            tfidf_scores: TF-IDFåˆ†æ•°å­—å…¸
            
        Returns:
            ä½åŒºåˆ†åº¦è¯ç»„é›†åˆ
        """
        if not tfidf_scores:
            return set()
        
        dynamic_stopwords = set()
        
        # è®¡ç®—é¢‘ç‡é˜ˆå€¼
        all_dfs = [score.df for score in tfidf_scores.values()]
        max_df = max(all_dfs) if all_dfs else 0
        frequency_cutoff = max_df * self.frequency_threshold
        
        for phrase, score in tfidf_scores.items():
            # æ¡ä»¶1: TF-IDFåˆ†æ•°ä½äºé˜ˆå€¼ï¼ˆä½åŒºåˆ†åº¦ï¼‰
            low_tfidf = score.tfidf < self.tfidf_threshold
            
            # æ¡ä»¶2: æ–‡æ¡£é¢‘ç‡é«˜äºé˜ˆå€¼ï¼ˆé«˜é¢‘ï¼‰
            high_frequency = score.df >= frequency_cutoff
            
            # æ¡ä»¶3: ä¸æ˜¯å·²çŸ¥çš„é™æ€åœè¯
            not_static_stopword = phrase not in self.static_stopwords
            
            # å¿…é¡»åŒæ—¶æ»¡è¶³æ‰€æœ‰æ¡ä»¶
            if low_tfidf and high_frequency and not_static_stopword:
                dynamic_stopwords.add(phrase)
                self.logger.debug(f"Identified dynamic stopword: {phrase} (TF-IDF: {score.tfidf:.4f}, DF: {score.df}, cutoff: {frequency_cutoff:.1f})")
        
        self.logger.info(f"Identified {len(dynamic_stopwords)} low discrimination phrases")
        
        return dynamic_stopwords
    
    def merge_stopword_lists(self, dynamic_stopwords: Set[str]) -> Set[str]:
        """
        åˆå¹¶é™æ€å’ŒåŠ¨æ€åœè¯è¡¨
        
        æ ¹æ®éœ€æ±‚4.4ï¼Œå°†åŠ¨æ€åœè¯è¡¨ä¸äººå·¥åœè¯è¡¨åˆå¹¶ã€‚
        
        Args:
            dynamic_stopwords: åŠ¨æ€å‘ç°çš„åœè¯é›†åˆ
            
        Returns:
            åˆå¹¶åçš„åœè¯é›†åˆ
        """
        merged_stopwords = self.static_stopwords.copy()
        merged_stopwords.update(dynamic_stopwords)
        
        self.logger.info(f"Merged stopwords: {len(self.static_stopwords)} static + {len(dynamic_stopwords)} dynamic = {len(merged_stopwords)} total")
        
        return merged_stopwords
    
    def apply_stopword_filter(self, phrases: List[str], stopwords: Optional[Set[str]] = None) -> List[str]:
        """
        åº”ç”¨åœè¯è¿‡æ»¤
        
        æ ¹æ®éœ€æ±‚4.5ï¼Œåº”ç”¨åˆå¹¶åçš„åœè¯è¡¨ä½œä¸ºå¼ºçº¦æŸã€‚
        
        Args:
            phrases: å¾…è¿‡æ»¤çš„è¯ç»„åˆ—è¡¨
            stopwords: åœè¯é›†åˆï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é™æ€åœè¯
            
        Returns:
            è¿‡æ»¤åçš„è¯ç»„åˆ—è¡¨
        """
        if stopwords is None:
            stopwords = self.static_stopwords
        
        filtered_phrases = []
        for phrase in tqdm(phrases, desc="ğŸ” Processing phrases", unit="phrase"):
            if phrase not in stopwords:
                filtered_phrases.append(phrase)
        
        removed_count = len(phrases) - len(filtered_phrases)
        if removed_count > 0:
            self.logger.debug(f"Filtered out {removed_count} stopword phrases")
        
        return filtered_phrases
    
    def get_stopword_explanation(self, phrase: str, tfidf_scores: Dict[str, TFIDFScore]) -> Dict[str, Any]:
        """
        è·å–åœè¯çš„å¯è§£é‡Šä¿¡æ¯
        
        æ ¹æ®éœ€æ±‚4.3ï¼Œå½¢æˆå¯è§£é‡Šçš„åŠ¨æ€stopwordåˆ—è¡¨ã€‚
        
        Args:
            phrase: è¯ç»„
            tfidf_scores: TF-IDFåˆ†æ•°å­—å…¸
            
        Returns:
            åœè¯è§£é‡Šä¿¡æ¯
        """
        explanation = {
            'phrase': phrase,
            'is_static_stopword': phrase in self.static_stopwords,
            'is_dynamic_stopword': False,
            'reason': []
        }
        
        if phrase in tfidf_scores:
            score = tfidf_scores[phrase]
            explanation.update({
                'tfidf_score': score.tfidf,
                'document_frequency': score.df,
                'term_frequency': score.tf,
                'inverse_document_frequency': score.idf
            })
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºåŠ¨æ€åœè¯
            max_df = max([s.df for s in tfidf_scores.values()]) if tfidf_scores else 0
            frequency_cutoff = max_df * self.frequency_threshold
            
            if score.tfidf < self.tfidf_threshold:
                explanation['reason'].append(f"Low TF-IDF score ({score.tfidf:.4f} < {self.tfidf_threshold})")
                
            if score.df >= frequency_cutoff:
                explanation['reason'].append(f"High document frequency ({score.df} >= {frequency_cutoff:.1f})")
                
            explanation['is_dynamic_stopword'] = (
                score.tfidf < self.tfidf_threshold and 
                score.df >= frequency_cutoff and 
                phrase not in self.static_stopwords
            )
        
        if phrase in self.static_stopwords:
            explanation['reason'].append("Present in static stopword list")
        
        return explanation
    
    def save_stopword_analysis(self, result: StopwordDiscoveryResult, output_path: str) -> None:
        """
        ä¿å­˜åœè¯åˆ†æç»“æœ
        
        Args:
            result: åœè¯å‘ç°ç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            output_data = result.to_dict()
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"Saved stopword analysis to {output_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to save stopword analysis: {e}")
            raise