"""
è¯ç»„æŠ½å–å™¨æ¨¡å—

å®ç°è¯ç»„/çŸ­è¯­æŠ½å–åŠŸèƒ½ï¼ŒåŒ…æ‹¬è‹±æ–‡2-gramæŠ½å–ã€ä¸­æ–‡çŸ­è¯­æŠ½å–å’Œç»Ÿè®¡çº¦æŸç­›é€‰ã€‚
æ ¹æ®éœ€æ±‚3.1ã€3.2ã€3.3ã€3.6å®ç°è¯ç»„çº§èŠ‚ç‚¹å•ä½å‡çº§ã€‚
"""

import re
import math
import logging
from tqdm import tqdm
from collections import Counter, defaultdict
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass

# å¯¼å…¥NLTK
try:
    import nltk
    from nltk.util import ngrams
    from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    logging.warning("NLTK not available, using basic n-gram extraction")

from ..core.data_models import ProcessedDocument, Phrase, Window
from ..core.config import Config
from ..core.error_handler import ErrorHandler


@dataclass
class StatisticalScores:
    """ç»Ÿè®¡åˆ†æ•°æ•°æ®ç»“æ„"""
    mutual_information: float = 0.0
    t_score: float = 0.0
    cohesion_score: float = 0.0
    frequency: int = 0
    left_entropy: float = 0.0
    right_entropy: float = 0.0


@dataclass
class PhraseCandidate:
    """è¯ç»„å€™é€‰æ•°æ®ç»“æ„"""
    text: str
    tokens: List[str]
    frequency: int
    statistical_scores: StatisticalScores
    language: str
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if not self.tokens:
            self.tokens = self.text.split()


class EnglishBigramExtractor:
    """
    è‹±æ–‡2-gramæŠ½å–å™¨
    
    æ ¹æ®éœ€æ±‚3.1ï¼Œé‡‡ç”¨2-gramï¼ˆbigramï¼‰ä½œä¸ºä¸»è¦èŠ‚ç‚¹å€™é€‰ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.min_frequency = config.get('min_phrase_frequency', 3)
        self.use_nltk_collocations = config.get('use_nltk_collocations', True) and NLTK_AVAILABLE
        
        # è¿‡æ»¤æ¨¡å¼
        self.filter_patterns = [
            r'^[^a-zA-Z]',  # ä¸ä»¥å­—æ¯å¼€å¤´
            r'[^a-zA-Z]$',  # ä¸ä»¥å­—æ¯ç»“å°¾
            r'^\d+$',       # çº¯æ•°å­—
            r'^.{1,2}$'     # å¤ªçŸ­çš„è¯ç»„
        ]
        
        logging.debug(f"EnglishBigramExtractor initialized with min_frequency={self.min_frequency}")
    
    def extract_bigrams(self, tokens: List[str]) -> List[PhraseCandidate]:
        """
        æŠ½å–è‹±æ–‡2-gram
        
        Args:
            tokens: åˆ†è¯ç»“æœ
            
        Returns:
            List[PhraseCandidate]: 2-gramå€™é€‰åˆ—è¡¨
        """
        if len(tokens) < 2:
            return []
        
        try:
            if self.use_nltk_collocations:
                return self._extract_with_nltk(tokens)
            else:
                return self._extract_basic_bigrams(tokens)
        except Exception as e:
            logging.warning(f"Bigram extraction failed: {e}, falling back to basic method")
            return self._extract_basic_bigrams(tokens)
    
    def _extract_with_nltk(self, tokens: List[str]) -> List[PhraseCandidate]:
        """
        ä½¿ç”¨NLTKçš„æ­é…å‘ç°å™¨æŠ½å–2-gram
        
        Args:
            tokens: åˆ†è¯ç»“æœ
            
        Returns:
            List[PhraseCandidate]: 2-gramå€™é€‰åˆ—è¡¨
        """
        # åˆ›å»ºæ­é…å‘ç°å™¨
        finder = BigramCollocationFinder.from_words(tokens)
        
        # åº”ç”¨é¢‘ç‡è¿‡æ»¤
        finder.apply_freq_filter(self.min_frequency)
        
        # è·å–2-gramåŠå…¶é¢‘ç‡
        bigram_freq = finder.ngram_fd
        
        candidates = []
        for (word1, word2), freq in bigram_freq.items():
            bigram_text = f"{word1} {word2}"
            
            # åº”ç”¨è¿‡æ»¤è§„åˆ™
            if self._should_filter_bigram(bigram_text):
                continue
            
            # åˆ›å»ºå€™é€‰
            candidate = PhraseCandidate(
                text=bigram_text,
                tokens=[word1, word2],
                frequency=freq,
                statistical_scores=StatisticalScores(frequency=freq),
                language='english'
            )
            
            candidates.append(candidate)
        
        logging.debug(f"Extracted {len(candidates)} English bigrams using NLTK")
        return candidates
    
    def _extract_basic_bigrams(self, tokens: List[str]) -> List[PhraseCandidate]:
        """
        åŸºç¡€2-gramæŠ½å–æ–¹æ³•
        
        Args:
            tokens: åˆ†è¯ç»“æœ
            
        Returns:
            List[PhraseCandidate]: 2-gramå€™é€‰åˆ—è¡¨
        """
        # ç”Ÿæˆæ‰€æœ‰2-gram
        if NLTK_AVAILABLE:
            bigrams = list(ngrams(tokens, 2))
        else:
            bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]
        
        # ç»Ÿè®¡é¢‘ç‡
        bigram_counts = Counter(bigrams)
        
        candidates = []
        for (word1, word2), freq in bigram_counts.items():
            if freq < self.min_frequency:
                continue
            
            bigram_text = f"{word1} {word2}"
            
            # åº”ç”¨è¿‡æ»¤è§„åˆ™
            if self._should_filter_bigram(bigram_text):
                continue
            
            # åˆ›å»ºå€™é€‰
            candidate = PhraseCandidate(
                text=bigram_text,
                tokens=[word1, word2],
                frequency=freq,
                statistical_scores=StatisticalScores(frequency=freq),
                language='english'
            )
            
            candidates.append(candidate)
        
        logging.debug(f"Extracted {len(candidates)} English bigrams using basic method")
        return candidates
    
    def _should_filter_bigram(self, bigram_text: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿‡æ»¤è¯¥2-gram
        
        Args:
            bigram_text: 2-gramæ–‡æœ¬
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥è¿‡æ»¤
        """
        for pattern in self.filter_patterns:
            if re.search(pattern, bigram_text):
                return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åœè¯ï¼ˆåŸºç¡€åœè¯ï¼‰
        basic_stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        words = bigram_text.lower().split()
        if len(words) == 2 and (words[0] in basic_stopwords or words[1] in basic_stopwords):
            return True
        
        return False


class ChinesePhraseExtractor:
    """
    ä¸­æ–‡çŸ­è¯­æŠ½å–å™¨
    
    æ ¹æ®éœ€æ±‚3.3ï¼Œé‡‡ç”¨åˆ†è¯åå†è¿›è¡ŒçŸ­è¯­æŠ½å–ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.min_frequency = config.get('min_phrase_frequency', 3)
        self.max_phrase_length = config.get('max_phrase_length', 4)
        self.min_phrase_length = config.get('min_phrase_length', 2)
        
        # ä¸­æ–‡å­—ç¬¦æ¨¡å¼
        self.chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
        
        # è¿‡æ»¤æ¨¡å¼
        self.filter_patterns = [
            r'^\d+$',       # çº¯æ•°å­—
            r'^[a-zA-Z]+$', # çº¯è‹±æ–‡
            r'^.{1}$'       # å•å­—ç¬¦
        ]
        
        logging.debug(f"ChinesePhraseExtractor initialized with min_frequency={self.min_frequency}")
    
    def extract_phrases(self, tokens: List[str]) -> List[PhraseCandidate]:
        """
        æŠ½å–ä¸­æ–‡çŸ­è¯­
        
        Args:
            tokens: åˆ†è¯ç»“æœ
            
        Returns:
            List[PhraseCandidate]: ä¸­æ–‡çŸ­è¯­å€™é€‰åˆ—è¡¨
        """
        if len(tokens) < self.min_phrase_length:
            return []
        
        try:
            # ç”Ÿæˆä¸åŒé•¿åº¦çš„n-gram
            all_ngrams = []
            
            for n in range(self.min_phrase_length, min(self.max_phrase_length + 1, len(tokens) + 1)):
                if NLTK_AVAILABLE:
                    n_grams = list(ngrams(tokens, n))
                else:
                    n_grams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
                
                all_ngrams.extend(n_grams)
            
            # ç»Ÿè®¡é¢‘ç‡
            ngram_counts = Counter(all_ngrams)
            
            candidates = []
            for ngram_tuple, freq in ngram_counts.items():
                if freq < self.min_frequency:
                    continue
                
                phrase_text = ''.join(ngram_tuple)  # ä¸­æ–‡ä¸éœ€è¦ç©ºæ ¼è¿æ¥
                
                # åº”ç”¨è¿‡æ»¤è§„åˆ™
                if self._should_filter_phrase(phrase_text):
                    continue
                
                # åˆ›å»ºå€™é€‰
                candidate = PhraseCandidate(
                    text=phrase_text,
                    tokens=list(ngram_tuple),
                    frequency=freq,
                    statistical_scores=StatisticalScores(frequency=freq),
                    language='chinese'
                )
                
                candidates.append(candidate)
            
            logging.debug(f"Extracted {len(candidates)} Chinese phrases")
            return candidates
            
        except Exception as e:
            logging.error(f"Chinese phrase extraction failed: {e}")
            return []
    
    def _should_filter_phrase(self, phrase_text: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿‡æ»¤è¯¥ä¸­æ–‡çŸ­è¯­
        
        Args:
            phrase_text: çŸ­è¯­æ–‡æœ¬
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥è¿‡æ»¤
        """
        # åº”ç”¨åŸºç¡€è¿‡æ»¤è§„åˆ™
        for pattern in self.filter_patterns:
            if re.search(pattern, phrase_text):
                return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ä¸­æ–‡å­—ç¬¦
        chinese_chars = len(self.chinese_pattern.findall(phrase_text))
        if chinese_chars < len(phrase_text) * 0.5:  # è‡³å°‘50%æ˜¯ä¸­æ–‡å­—ç¬¦
            return True
        
        # æ£€æŸ¥åŸºç¡€åœè¯
        basic_stopwords = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'éƒ½', 'ä¸€', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä¼š', 'ç€'}
        if phrase_text in basic_stopwords:
            return True
        
        return False


class StatisticalFilter:
    """
    ç»Ÿè®¡çº¦æŸç­›é€‰å™¨
    
    æ ¹æ®éœ€æ±‚3.2ï¼Œç»“åˆç»Ÿè®¡çº¦æŸï¼ˆäº’ä¿¡æ¯ã€t-scoreæˆ–å‡å›ºåº¦ï¼‰ç­›é€‰é«˜è´¨é‡çŸ­è¯­ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # é˜ˆå€¼è®¾ç½®
        self.mi_threshold = config.get('mutual_information_threshold', 0.0)
        self.t_score_threshold = config.get('t_score_threshold', 2.0)
        self.cohesion_threshold = config.get('cohesion_threshold', 0.0)
        
        # å¯ç”¨çš„è¿‡æ»¤å™¨
        self.use_mutual_information = config.get('use_mutual_information', True)
        self.use_t_score = config.get('use_t_score', True)
        self.use_cohesion = config.get('use_cohesion', False)
        
        logging.debug("StatisticalFilter initialized")
    
    def filter_phrases(self, candidates: List[PhraseCandidate], corpus_stats: Dict[str, Any]) -> List[PhraseCandidate]:
        """
        åº”ç”¨ç»Ÿè®¡çº¦æŸç­›é€‰çŸ­è¯­
        
        Args:
            candidates: å€™é€‰çŸ­è¯­åˆ—è¡¨
            corpus_stats: è¯­æ–™åº“ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            List[PhraseCandidate]: ç­›é€‰åçš„çŸ­è¯­åˆ—è¡¨
        """
        if not candidates:
            return []
        
        try:
            # è®¡ç®—ç»Ÿè®¡åˆ†æ•°
            for candidate in candidates:
                self._calculate_statistical_scores(candidate, corpus_stats)
            
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            filtered_candidates = []
            for candidate in candidates:
                if self._passes_statistical_filters(candidate):
                    filtered_candidates.append(candidate)
            
            logging.debug(f"Statistical filtering: {len(candidates)} -> {len(filtered_candidates)} candidates")
            return filtered_candidates
            
        except Exception as e:
            logging.error(f"Statistical filtering failed: {e}")
            return candidates  # è¿”å›åŸå§‹å€™é€‰åˆ—è¡¨
    
    def _calculate_statistical_scores(self, candidate: PhraseCandidate, corpus_stats: Dict[str, Any]) -> None:
        """
        è®¡ç®—ç»Ÿè®¡åˆ†æ•°
        
        Args:
            candidate: å€™é€‰çŸ­è¯­
            corpus_stats: è¯­æ–™åº“ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # è·å–å¿…è¦çš„ç»Ÿè®¡ä¿¡æ¯
            total_tokens = corpus_stats.get('total_tokens', 1)
            token_counts = corpus_stats.get('token_counts', {})
            bigram_counts = corpus_stats.get('bigram_counts', {})
            
            if len(candidate.tokens) == 2:
                # è®¡ç®—2-gramçš„ç»Ÿè®¡åˆ†æ•°
                word1, word2 = candidate.tokens
                
                # é¢‘ç‡ä¿¡æ¯
                freq_w1 = token_counts.get(word1, 1)
                freq_w2 = token_counts.get(word2, 1)
                freq_w1_w2 = candidate.frequency
                
                # äº’ä¿¡æ¯ (Mutual Information)
                if self.use_mutual_information:
                    mi = self._calculate_mutual_information(freq_w1, freq_w2, freq_w1_w2, total_tokens)
                    candidate.statistical_scores.mutual_information = mi
                
                # t-score
                if self.use_t_score:
                    t_score = self._calculate_t_score(freq_w1, freq_w2, freq_w1_w2, total_tokens)
                    candidate.statistical_scores.t_score = t_score
                
                # å‡å›ºåº¦ (Cohesion)
                if self.use_cohesion:
                    cohesion = self._calculate_cohesion_score(candidate, corpus_stats)
                    candidate.statistical_scores.cohesion_score = cohesion
            
            else:
                # å¯¹äºæ›´é•¿çš„çŸ­è¯­ï¼Œä½¿ç”¨ç®€åŒ–çš„ç»Ÿè®¡æ–¹æ³•
                candidate.statistical_scores.mutual_information = math.log(candidate.frequency + 1)
                candidate.statistical_scores.t_score = math.sqrt(candidate.frequency)
                candidate.statistical_scores.cohesion_score = candidate.frequency / total_tokens
                
        except Exception as e:
            logging.warning(f"Failed to calculate statistical scores for '{candidate.text}': {e}")
            # è®¾ç½®é»˜è®¤åˆ†æ•°
            candidate.statistical_scores.mutual_information = 0.0
            candidate.statistical_scores.t_score = 0.0
            candidate.statistical_scores.cohesion_score = 0.0
    
    def _calculate_mutual_information(self, freq_w1: int, freq_w2: int, freq_w1_w2: int, total_tokens: int) -> float:
        """
        è®¡ç®—äº’ä¿¡æ¯
        
        MI(w1, w2) = log(P(w1, w2) / (P(w1) * P(w2)))
        """
        try:
            p_w1 = freq_w1 / total_tokens
            p_w2 = freq_w2 / total_tokens
            p_w1_w2 = freq_w1_w2 / (total_tokens - 1)  # å‡1å› ä¸ºæ˜¯bigram
            
            if p_w1 > 0 and p_w2 > 0 and p_w1_w2 > 0:
                mi = math.log(p_w1_w2 / (p_w1 * p_w2))
                return mi
            else:
                return 0.0
        except (ValueError, ZeroDivisionError):
            return 0.0
    
    def _calculate_t_score(self, freq_w1: int, freq_w2: int, freq_w1_w2: int, total_tokens: int) -> float:
        """
        è®¡ç®—t-score
        
        t-score = (P(w1, w2) - P(w1) * P(w2)) / sqrt(P(w1, w2) / N)
        """
        try:
            p_w1 = freq_w1 / total_tokens
            p_w2 = freq_w2 / total_tokens
            p_w1_w2 = freq_w1_w2 / (total_tokens - 1)
            
            if p_w1_w2 > 0:
                numerator = p_w1_w2 - (p_w1 * p_w2)
                denominator = math.sqrt(p_w1_w2 / (total_tokens - 1))
                
                if denominator > 0:
                    t_score = numerator / denominator
                    return t_score
                else:
                    return 0.0
            else:
                return 0.0
        except (ValueError, ZeroDivisionError):
            return 0.0
    
    def _calculate_cohesion_score(self, candidate: PhraseCandidate, corpus_stats: Dict[str, Any]) -> float:
        """
        è®¡ç®—å‡å›ºåº¦åˆ†æ•°
        
        ç®€åŒ–çš„å‡å›ºåº¦è®¡ç®—ï¼ŒåŸºäºå·¦å³é‚»æ¥è¯çš„ç†µ
        """
        try:
            # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„å‡å›ºåº¦è®¡ç®—
            # å®é™…å®ç°ä¸­å¯ä»¥åŸºäºå·¦å³é‚»æ¥è¯çš„åˆ†å¸ƒè®¡ç®—ç†µ
            return candidate.frequency / corpus_stats.get('total_tokens', 1)
        except Exception:
            return 0.0
    
    def _passes_statistical_filters(self, candidate: PhraseCandidate) -> bool:
        """
        æ£€æŸ¥å€™é€‰çŸ­è¯­æ˜¯å¦é€šè¿‡ç»Ÿè®¡è¿‡æ»¤æ¡ä»¶
        
        Args:
            candidate: å€™é€‰çŸ­è¯­
            
        Returns:
            bool: æ˜¯å¦é€šè¿‡è¿‡æ»¤
        """
        scores = candidate.statistical_scores
        
        # äº’ä¿¡æ¯è¿‡æ»¤
        if self.use_mutual_information and scores.mutual_information < self.mi_threshold:
            return False
        
        # t-scoreè¿‡æ»¤
        if self.use_t_score and scores.t_score < self.t_score_threshold:
            return False
        
        # å‡å›ºåº¦è¿‡æ»¤
        if self.use_cohesion and scores.cohesion_score < self.cohesion_threshold:
            return False
        
        return True


class PhraseExtractor:
    """
    è¯ç»„æŠ½å–å™¨ä¸»ç±»
    
    æ ¹æ®éœ€æ±‚3.1ã€3.2ã€3.3ã€3.6å®ç°è¯ç»„/çŸ­è¯­æŠ½å–åŠŸèƒ½ã€‚
    """
    
    def __init__(self, config: Optional[Config] = None):
        """
        åˆå§‹åŒ–è¯ç»„æŠ½å–å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config or Config()
        self.error_handler = ErrorHandler(self.config.get_section('error_handling'))
        
        # è·å–è¯ç»„æŠ½å–é…ç½®
        phrase_config = self.config.get_section('phrase_extraction')
        
        # åˆå§‹åŒ–å­ç»„ä»¶
        self.english_extractor = EnglishBigramExtractor(phrase_config)
        self.chinese_extractor = ChinesePhraseExtractor(phrase_config)
        self.statistical_filter = StatisticalFilter(phrase_config)
        
        # æ‰©å±•é€‰é¡¹
        self.enable_multiword_extension = phrase_config.get('enable_multiword_extension', True)
        self.max_phrase_length = phrase_config.get('max_phrase_length', 4)
        
        logging.info("PhraseExtractor initialized successfully")
    
    def extract_phrases_from_document(self, processed_doc: ProcessedDocument) -> ProcessedDocument:
        """
        ä»å¤„ç†åçš„æ–‡æ¡£ä¸­æŠ½å–çŸ­è¯­
        
        Args:
            processed_doc: å¤„ç†åçš„æ–‡æ¡£
            
        Returns:
            ProcessedDocument: åŒ…å«çŸ­è¯­ä¿¡æ¯çš„æ–‡æ¡£
        """
        try:
            language = processed_doc.original_doc.language or 'english'
            tokens = processed_doc.tokens
            
            if not tokens:
                logging.warning(f"No tokens found in document {processed_doc.original_doc.segment_id}")
                return processed_doc
            
            # æ ¹æ®è¯­è¨€é€‰æ‹©æŠ½å–æ–¹æ³•
            if language == 'english':
                candidates = self.english_extractor.extract_bigrams(tokens)
            elif language == 'chinese':
                candidates = self.chinese_extractor.extract_phrases(tokens)
            else:
                logging.warning(f"Unknown language {language}, using English extractor")
                candidates = self.english_extractor.extract_bigrams(tokens)
            
            # è½¬æ¢ä¸ºçŸ­è¯­åˆ—è¡¨
            phrases = [candidate.text for candidate in candidates]
            
            # æ›´æ–°æ–‡æ¡£çš„çŸ­è¯­ä¿¡æ¯
            processed_doc.phrases = phrases
            
            # æ›´æ–°çª—å£ä¸­çš„çŸ­è¯­ä¿¡æ¯
            if processed_doc.windows:
                processed_doc.windows[0].phrases = phrases
            
            # æ›´æ–°å…ƒæ•°æ®
            processed_doc.processing_metadata.update({
                'phrase_count': len(phrases),
                'phrase_extraction_method': language,
                'candidates_before_filtering': len(candidates)
            })
            
            logging.debug(f"Extracted {len(phrases)} phrases from document {processed_doc.original_doc.segment_id}")
            return processed_doc
            
        except Exception as e:
            error_msg = f"Failed to extract phrases from document {processed_doc.original_doc.segment_id}: {e}"
            logging.error(error_msg)
            return self.error_handler.handle_processing_error(e, f"extract_phrases:{processed_doc.original_doc.segment_id}")
    
    def extract_phrases_from_tokens(self, tokens: List[str], language: str = 'english') -> List[str]:
        """
        ä»åˆ†è¯ç»“æœä¸­æŠ½å–çŸ­è¯­
        
        Args:
            tokens: åˆ†è¯ç»“æœ
            language: è¯­è¨€ç±»å‹
            
        Returns:
            List[str]: çŸ­è¯­åˆ—è¡¨
        """
        try:
            if language == 'english':
                candidates = self.english_extractor.extract_bigrams(tokens)
            elif language == 'chinese':
                candidates = self.chinese_extractor.extract_phrases(tokens)
            else:
                candidates = self.english_extractor.extract_bigrams(tokens)
            
            return [candidate.text for candidate in candidates]
            
        except Exception as e:
            logging.error(f"Failed to extract phrases from tokens: {e}")
            return []
    
    def apply_statistical_filtering(self, candidates: List[PhraseCandidate], corpus_stats: Dict[str, Any]) -> List[PhraseCandidate]:
        """
        åº”ç”¨ç»Ÿè®¡çº¦æŸç­›é€‰
        
        Args:
            candidates: å€™é€‰çŸ­è¯­åˆ—è¡¨
            corpus_stats: è¯­æ–™åº“ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            List[PhraseCandidate]: ç­›é€‰åçš„çŸ­è¯­åˆ—è¡¨
        """
        try:
            return self.statistical_filter.filter_phrases(candidates, corpus_stats)
        except Exception as e:
            logging.error(f"Statistical filtering failed: {e}")
            return candidates
    
    def batch_extract_phrases(self, processed_docs: List[ProcessedDocument]) -> List[ProcessedDocument]:
        """
        æ‰¹é‡æŠ½å–çŸ­è¯­
        
        Args:
            processed_docs: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            List[ProcessedDocument]: åŒ…å«çŸ­è¯­ä¿¡æ¯çš„æ–‡æ¡£åˆ—è¡¨
        """
        updated_docs = []
        
        for i, doc in enumerate(processed_docs):
            try:
                updated_doc = self.extract_phrases_from_document(doc)
                updated_docs.append(updated_doc)
                
                if (i + 1) % 100 == 0:
                    logging.info(f"Extracted phrases from {i + 1}/{len(processed_docs)} documents")
                    
            except Exception as e:
                logging.error(f"Failed to extract phrases from document {doc.original_doc.segment_id}: {e}")
                # ç»§ç»­å¤„ç†å…¶ä»–æ–‡æ¡£
                updated_docs.append(doc)
                continue
        
        logging.info(f"Batch phrase extraction completed: {len(updated_docs)} documents processed")
        return updated_docs
    
    def calculate_corpus_statistics(self, processed_docs: List[ProcessedDocument]) -> Dict[str, Any]:
        """
        è®¡ç®—è¯­æ–™åº“ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            processed_docs: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            Dict[str, Any]: è¯­æ–™åº“ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            all_tokens = []
            all_bigrams = []
            
            for doc in tqdm(processed_docs, desc="ğŸ“„ Processing documents", unit="doc"):
                all_tokens.extend(doc.tokens)
                
                # ç”Ÿæˆbigramsç”¨äºç»Ÿè®¡
                tokens = doc.tokens
                if len(tokens) >= 2:
                    if NLTK_AVAILABLE:
                        bigrams = list(ngrams(tokens, 2))
                    else:
                        bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]
                    all_bigrams.extend(bigrams)
            
            # ç»Ÿè®¡é¢‘ç‡
            token_counts = Counter(all_tokens)
            bigram_counts = Counter(all_bigrams)
            
            stats = {
                'total_tokens': len(all_tokens),
                'unique_tokens': len(token_counts),
                'total_bigrams': len(all_bigrams),
                'unique_bigrams': len(bigram_counts),
                'token_counts': dict(token_counts),
                'bigram_counts': dict(bigram_counts),
                'average_tokens_per_doc': len(all_tokens) / len(processed_docs) if processed_docs else 0
            }
            
            logging.debug(f"Calculated corpus statistics: {stats['total_tokens']} tokens, {stats['unique_tokens']} unique tokens")
            return stats
            
        except Exception as e:
            logging.error(f"Failed to calculate corpus statistics: {e}")
            return {}
    
    def get_extraction_statistics(self, processed_docs: List[ProcessedDocument]) -> Dict[str, Any]:
        """
        è·å–çŸ­è¯­æŠ½å–ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            processed_docs: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            Dict[str, Any]: æŠ½å–ç»Ÿè®¡ä¿¡æ¯
        """
        if not processed_docs:
            return {}
        
        # è¯­è¨€åˆ†å¸ƒ
        language_counts = {}
        total_phrases = 0
        phrase_length_dist = Counter()
        
        for doc in tqdm(processed_docs, desc="ğŸ“„ Processing documents", unit="doc"):
            lang = doc.original_doc.language or 'unknown'
            language_counts[lang] = language_counts.get(lang, 0) + 1
            
            phrases = doc.phrases
            total_phrases += len(phrases)
            
            # ç»Ÿè®¡çŸ­è¯­é•¿åº¦åˆ†å¸ƒ
            for phrase in tqdm(phrases, desc="ğŸ” Processing phrases", unit="phrase"):
                length = len(phrase.split()) if doc.original_doc.language == 'english' else len(phrase)
                phrase_length_dist[length] += 1
        
        return {
            'total_documents': len(processed_docs),
            'language_distribution': language_counts,
            'total_phrases': total_phrases,
            'average_phrases_per_doc': total_phrases / len(processed_docs),
            'phrase_length_distribution': dict(phrase_length_dist),
            'unique_phrases': len(set(phrase for doc in processed_docs for phrase in doc.phrases))
        }