"""
æ€»å›¾æ„å»ºå™¨ï¼ˆGlobalGraphBuilderï¼‰

æ ¹æ®éœ€æ±‚2.1ã€2.3ã€2.5ã€5.4ã€5.5å®ç°å…¨å±€å…±ç°å›¾çš„æ„å»ºåŠŸèƒ½ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
- åˆ›å»ºç»Ÿä¸€è¯è¡¨ç”ŸæˆåŠŸèƒ½
- å®ç°å…±ç°çŸ©é˜µè®¡ç®—  
- é›†æˆEasyGraphå›¾æ„å»º
- æ·»åŠ å­¤ç«‹èŠ‚ç‚¹ä¿ç•™æœºåˆ¶
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'Easy-Graph'))

from typing import Dict, List, Set, Tuple, Any, Optional
import numpy as np
import scipy.sparse
from collections import defaultdict, Counter
from itertools import combinations
import logging
from tqdm import tqdm
from datetime import datetime

try:
    import easygraph as eg
except ImportError:
    print("Warning: EasyGraph not available. Some functionality may be limited.")
    eg = None

from ..core.data_models import (
    ProcessedDocument, 
    GlobalGraph, 
    Window, 
    create_phrase_mapping
)
from ..core.logger import setup_logger


class GlobalGraphBuilder:
    """
    æ€»å›¾æ„å»ºå™¨
    
    æ ¹æ®éœ€æ±‚2.1å®ç°ä»æ‰€æœ‰æ–‡æ¡£æ„å»ºç»Ÿä¸€çš„æ€»å›¾ï¼ˆGlobal_Graphï¼‰ã€‚
    é‡‡ç”¨"æ€»å›¾ä¼˜å…ˆ"ç­–ç•¥ï¼Œç¡®ä¿è·¨å·å¯¹æ¯”æ—¶æŒ‡æ ‡å¯æ¯”ã€èŠ‚ç‚¹è¯­ä¹‰ç©ºé—´ä¸€è‡´ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ€»å›¾æ„å»ºå™¨
        
        Args:
            config: é…ç½®å‚æ•°å­—å…¸
        """
        self.config = config
        self.logger = setup_logger(__name__)
        
        # é…ç½®å‚æ•°
        self.window_type = config.get('window_type', 'segment')
        self.edge_weight_method = config.get('edge_weight_method', 'binary')
        self.preserve_isolated_nodes = config.get('preserve_isolated_nodes', True)
        self.min_cooccurrence_count = config.get('min_cooccurrence_count', 1)
        
        # å†…éƒ¨çŠ¶æ€
        self._phrase_counter = Counter()
        self._cooccurrence_counter = defaultdict(int)
        self._all_phrases = set()
        
        self.logger.info(f"GlobalGraphBuilder initialized with config: {config}")
    
    def build_global_graph(self, processed_docs: List[ProcessedDocument]) -> GlobalGraph:
        """
        æ„å»ºå…¨å±€å…±ç°å›¾
        
        æ ¹æ®éœ€æ±‚2.1ä»æ‰€æœ‰æ–‡æ¡£æ„å»ºç»Ÿä¸€çš„æ€»å›¾ã€‚
        
        Args:
            processed_docs: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            GlobalGraph: æ„å»ºçš„å…¨å±€å›¾å¯¹è±¡
            
        Raises:
            ValueError: å½“è¾“å…¥æ–‡æ¡£ä¸ºç©ºæ—¶
        """
        if not processed_docs:
            raise ValueError("Cannot build global graph from empty document list")
        
        self.logger.info(f"Building global graph from {len(processed_docs)} documents")
        
        # æ­¥éª¤1: åˆ›å»ºç»Ÿä¸€è¯è¡¨
        vocabulary, reverse_vocabulary = self._create_unified_vocabulary(processed_docs)
        self.logger.info(f"Created unified vocabulary with {len(vocabulary)} phrases")
        
        # æ­¥éª¤2: è®¡ç®—å…±ç°çŸ©é˜µ
        cooccurrence_matrix = self._calculate_cooccurrence_matrix(processed_docs, vocabulary)
        self.logger.info(f"Calculated cooccurrence matrix: {cooccurrence_matrix.shape}")
        
        # æ­¥éª¤3: æ„å»ºEasyGraphå®ä¾‹
        easygraph_instance = self._build_easygraph_instance(vocabulary, cooccurrence_matrix)
        
        # æ­¥éª¤4: åˆ›å»ºGlobalGraphå¯¹è±¡
        global_graph = GlobalGraph(
            vocabulary=vocabulary,
            reverse_vocabulary=reverse_vocabulary,
            cooccurrence_matrix=cooccurrence_matrix,
            easygraph_instance=easygraph_instance,
            metadata={
                'created_at': self._get_timestamp(),
                'node_count': len(vocabulary),
                'edge_count': cooccurrence_matrix.nnz,
                'density': cooccurrence_matrix.nnz / (len(vocabulary) * (len(vocabulary) - 1)) if len(vocabulary) > 1 else 0.0,
                'isolated_nodes_preserved': self.preserve_isolated_nodes,
                'edge_weight_method': self.edge_weight_method,
                'source_documents': len(processed_docs)
            }
        )
        
        self.logger.info(f"Global graph built successfully: {len(vocabulary)} nodes, {cooccurrence_matrix.nnz} edges")
        return global_graph
    
    def _create_unified_vocabulary(self, processed_docs: List[ProcessedDocument]) -> Tuple[Dict[str, int], Dict[int, str]]:
        """
        åˆ›å»ºç»Ÿä¸€è¯è¡¨ç”ŸæˆåŠŸèƒ½
        
        æ ¹æ®éœ€æ±‚2.3å’Œ5.4ï¼Œç”¨äºç»Ÿä¸€è¯è¡¨ä¸èŠ‚ç‚¹ç©ºé—´ï¼Œç¡®ä¿èŠ‚ç‚¹æ˜ å°„çš„å”¯ä¸€æ€§ã€‚
        
        Args:
            processed_docs: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            Tuple[Dict[str, int], Dict[int, str]]: è¯è¡¨æ˜ å°„å’Œåå‘æ˜ å°„
        """
        self.logger.debug("Creating unified vocabulary from all documents")
        
        # æ”¶é›†æ‰€æœ‰è¯ç»„
        all_phrases = set()
        phrase_frequencies = Counter()
        
        for doc in tqdm(processed_docs, desc="ğŸ“„ Processing documents", unit="doc"):
            for window in doc.windows:
                for phrase in window.phrases:
                    all_phrases.add(phrase)
                    phrase_frequencies[phrase] += 1
        
        # æŒ‰é¢‘ç‡æ’åºï¼Œç¡®ä¿é«˜é¢‘è¯ç»„è·å¾—è¾ƒå°çš„ID
        sorted_phrases = sorted(all_phrases, key=lambda p: (-phrase_frequencies[p], p))
        
        # åˆ›å»ºæ˜ å°„
        vocabulary = {}
        reverse_vocabulary = {}
        
        for i, phrase in enumerate(sorted_phrases):
            vocabulary[phrase] = i
            reverse_vocabulary[i] = phrase
        
        self.logger.debug(f"Created vocabulary with {len(vocabulary)} unique phrases")
        return vocabulary, reverse_vocabulary
    
    def _calculate_cooccurrence_matrix(self, processed_docs: List[ProcessedDocument], 
                                     vocabulary: Dict[str, int]) -> scipy.sparse.csr_matrix:
        """
        å®ç°å…±ç°çŸ©é˜µè®¡ç®—
        
        æ ¹æ®éœ€æ±‚5.2å’Œ5.5ï¼Œå°†æ¯ä¸ªsegmentçš„textè§†ä¸ºä¸€ä¸ªå…±ç°çª—å£ï¼Œ
        è®¡ç®—æ— å‘ã€å¸¦æƒçš„å…±ç°å…³ç³»ã€‚
        
        Args:
            processed_docs: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨
            vocabulary: è¯è¡¨æ˜ å°„
            
        Returns:
            scipy.sparse.csr_matrix: å…±ç°çŸ©é˜µ
        """
        self.logger.debug("Calculating cooccurrence matrix")
        
        vocab_size = len(vocabulary)
        cooccurrence_counts = defaultdict(int)
        
        # ç»Ÿè®¡å…±ç°å…³ç³»
        total_windows = 0
        for doc in tqdm(processed_docs, desc="ğŸ“„ Processing documents", unit="doc"):
            for window in doc.windows:
                if len(window.phrases) < 2:
                    continue
                
                total_windows += 1
                
                # è·å–çª—å£ä¸­æ‰€æœ‰è¯ç»„çš„èŠ‚ç‚¹ID
                phrase_ids = []
                for phrase in window.phrases:
                    if phrase in vocabulary:
                        phrase_ids.append(vocabulary[phrase])
                
                # è®¡ç®—çª—å£å†…æ‰€æœ‰è¯ç»„å¯¹çš„å…±ç°
                for i, j in combinations(phrase_ids, 2):
                    # ç¡®ä¿æ— å‘æ€§ï¼šè¾ƒå°çš„IDåœ¨å‰
                    if i > j:
                        i, j = j, i
                    
                    if self.edge_weight_method == 'binary':
                        cooccurrence_counts[(i, j)] += 1
                    elif self.edge_weight_method == 'frequency':
                        # å¯ä»¥æ ¹æ®è¯ç»„åœ¨çª—å£ä¸­çš„é¢‘ç‡åŠ æƒ
                        cooccurrence_counts[(i, j)] += 1
        
        self.logger.debug(f"Processed {total_windows} windows, found {len(cooccurrence_counts)} cooccurrence pairs")
        
        # è¿‡æ»¤ä½é¢‘å…±ç°
        filtered_cooccurrences = {
            (i, j): count for (i, j), count in cooccurrence_counts.items()
            if count >= self.min_cooccurrence_count
        }
        
        # æ„å»ºç¨€ç–çŸ©é˜µ
        if filtered_cooccurrences:
            rows, cols, data = zip(*[
                (i, j, count) for (i, j), count in filtered_cooccurrences.items()
            ])
            
            # åˆ›å»ºå¯¹ç§°çŸ©é˜µ
            all_rows = list(rows) + list(cols)
            all_cols = list(cols) + list(rows)
            all_data = list(data) + list(data)
            
            cooccurrence_matrix = scipy.sparse.csr_matrix(
                (all_data, (all_rows, all_cols)),
                shape=(vocab_size, vocab_size)
            )
        else:
            # ç©ºçŸ©é˜µ
            cooccurrence_matrix = scipy.sparse.csr_matrix((vocab_size, vocab_size))
        
        self.logger.debug(f"Created cooccurrence matrix: {cooccurrence_matrix.shape}, {cooccurrence_matrix.nnz} non-zero entries")
        return cooccurrence_matrix
    
    def _build_easygraph_instance(self, vocabulary: Dict[str, int], 
                                cooccurrence_matrix: scipy.sparse.csr_matrix) -> Optional[Any]:
        """
        é›†æˆEasyGraphå›¾æ„å»º
        
        æ ¹æ®éœ€æ±‚2.5ï¼Œæ˜¾å¼ä¿ç•™å­¤ç«‹èŠ‚ç‚¹ä»¥åæ˜ ç¼ºå¤±æˆ–å¼±è¿æ¥ç°è±¡ã€‚
        
        Args:
            vocabulary: è¯è¡¨æ˜ å°„
            cooccurrence_matrix: å…±ç°çŸ©é˜µ
            
        Returns:
            Optional[easygraph.Graph]: EasyGraphå›¾å®ä¾‹ï¼Œå¦‚æœEasyGraphä¸å¯ç”¨åˆ™è¿”å›None
        """
        if eg is None:
            self.logger.warning("EasyGraph not available, skipping graph instance creation")
            return None
        
        self.logger.debug("Building EasyGraph instance")
        
        # åˆ›å»ºæ— å‘å›¾
        graph = eg.Graph()
        
        # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬å­¤ç«‹èŠ‚ç‚¹ï¼‰
        for phrase, node_id in vocabulary.items():
            graph.add_node(node_id, phrase=phrase)
        
        self.logger.debug(f"Added {len(vocabulary)} nodes to graph")
        
        # æ·»åŠ è¾¹
        coo_matrix = cooccurrence_matrix.tocoo()
        edge_count = 0
        
        for i, j, weight in zip(coo_matrix.row, coo_matrix.col, coo_matrix.data):
            if i < j:  # é¿å…é‡å¤æ·»åŠ è¾¹ï¼ˆå› ä¸ºçŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼‰
                graph.add_edge(i, j, weight=float(weight))
                edge_count += 1
        
        self.logger.debug(f"Added {edge_count} edges to graph")
        
        # éªŒè¯å­¤ç«‹èŠ‚ç‚¹ä¿ç•™
        if self.preserve_isolated_nodes:
            degree_dict = graph.degree()
            isolated_nodes = [node for node in graph.nodes if degree_dict.get(node, 0) == 0]
            self.logger.info(f"Preserved {len(isolated_nodes)} isolated nodes")
        
        return graph
    
    def get_graph_statistics(self, global_graph: GlobalGraph) -> Dict[str, Any]:
        """
        è·å–å›¾ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            global_graph: å…¨å±€å›¾å¯¹è±¡
            
        Returns:
            Dict[str, Any]: å›¾ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'node_count': global_graph.get_node_count(),
            'vocabulary_size': len(global_graph.vocabulary)
        }
        
        if global_graph.cooccurrence_matrix is not None:
            matrix = global_graph.cooccurrence_matrix
            stats.update({
                'edge_count': matrix.nnz // 2,  # é™¤ä»¥2å› ä¸ºçŸ©é˜µæ˜¯å¯¹ç§°çš„
                'density': matrix.nnz / (matrix.shape[0] * (matrix.shape[0] - 1)) if matrix.shape[0] > 1 else 0.0,
                'sparsity': 1.0 - (matrix.nnz / (matrix.shape[0] * matrix.shape[1])) if matrix.shape[0] > 0 else 1.0
            })
        
        if global_graph.easygraph_instance is not None and eg is not None:
            graph = global_graph.easygraph_instance
            try:
                stats.update({
                    'connected_components': eg.number_connected_components(graph),
                    'isolated_nodes': len([n for n in graph.nodes if graph.degree().get(n, 0) == 0]),
                    'average_degree': sum(graph.degree().values()) / len(graph.nodes) if len(graph.nodes) > 0 else 0.0
                })
            except Exception as e:
                self.logger.warning(f"Failed to calculate advanced graph statistics: {e}")
        
        return stats
    
    def validate_graph_properties(self, global_graph: GlobalGraph) -> Dict[str, bool]:
        """
        éªŒè¯å›¾å±æ€§
        
        éªŒè¯æ„å»ºçš„å›¾æ˜¯å¦æ»¡è¶³éœ€æ±‚ä¸­çš„å„é¡¹å±æ€§ã€‚
        
        Args:
            global_graph: å…¨å±€å›¾å¯¹è±¡
            
        Returns:
            Dict[str, bool]: éªŒè¯ç»“æœ
        """
        validation_results = {}
        
        # éªŒè¯è¯è¡¨æ˜ å°„å”¯ä¸€æ€§ï¼ˆéœ€æ±‚5.4ï¼‰
        vocab_size = len(global_graph.vocabulary)
        reverse_vocab_size = len(global_graph.reverse_vocabulary)
        validation_results['vocabulary_mapping_unique'] = (vocab_size == reverse_vocab_size)
        
        # éªŒè¯å…±ç°å…³ç³»æ— å‘æ€§ï¼ˆéœ€æ±‚5.5ï¼‰
        if global_graph.cooccurrence_matrix is not None:
            matrix = global_graph.cooccurrence_matrix
            # æ£€æŸ¥çŸ©é˜µæ˜¯å¦å¯¹ç§°
            is_symmetric = np.allclose(matrix.toarray(), matrix.toarray().T)
            validation_results['cooccurrence_undirected'] = is_symmetric
        else:
            validation_results['cooccurrence_undirected'] = True
        
        # éªŒè¯å­¤ç«‹èŠ‚ç‚¹ä¿ç•™ï¼ˆéœ€æ±‚2.5ï¼‰
        if global_graph.easygraph_instance is not None and eg is not None:
            graph = global_graph.easygraph_instance
            total_nodes = len(graph.nodes)
            degree_dict = graph.degree()
            nodes_with_edges = len([n for n in graph.nodes if degree_dict.get(n, 0) > 0])
            isolated_nodes = total_nodes - nodes_with_edges
            
            # å¦‚æœé…ç½®è¦æ±‚ä¿ç•™å­¤ç«‹èŠ‚ç‚¹ï¼Œåˆ™åº”è¯¥æœ‰å®Œæ•´çš„èŠ‚ç‚¹é›†
            if self.preserve_isolated_nodes:
                validation_results['isolated_nodes_preserved'] = (total_nodes == vocab_size)
            else:
                validation_results['isolated_nodes_preserved'] = True
        else:
            validation_results['isolated_nodes_preserved'] = True
        
        return validation_results
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()


class CooccurrenceCalculator:
    """
    å…±ç°è®¡ç®—å™¨
    
    ä¸“é—¨è´Ÿè´£å…±ç°å…³ç³»çš„è®¡ç®—é€»è¾‘ã€‚
    """
    
    def __init__(self, window_type: str = 'segment', weight_method: str = 'binary'):
        """
        åˆå§‹åŒ–å…±ç°è®¡ç®—å™¨
        
        Args:
            window_type: çª—å£ç±»å‹ï¼Œé»˜è®¤ä¸º'segment'
            weight_method: æƒé‡è®¡ç®—æ–¹æ³•ï¼Œé»˜è®¤ä¸º'binary'
        """
        self.window_type = window_type
        self.weight_method = weight_method
        self.logger = setup_logger(__name__)
    
    def calculate_window_cooccurrences(self, window: Window, vocabulary: Dict[str, int]) -> List[Tuple[int, int, float]]:
        """
        è®¡ç®—å•ä¸ªçª—å£å†…çš„å…±ç°å…³ç³»
        
        Args:
            window: å…±ç°çª—å£
            vocabulary: è¯è¡¨æ˜ å°„
            
        Returns:
            List[Tuple[int, int, float]]: å…±ç°å…³ç³»åˆ—è¡¨ (node_i, node_j, weight)
        """
        if len(window.phrases) < 2:
            return []
        
        cooccurrences = []
        phrase_ids = []
        
        # è·å–çª—å£ä¸­æ‰€æœ‰æœ‰æ•ˆè¯ç»„çš„ID
        for phrase in window.phrases:
            if phrase in vocabulary:
                phrase_ids.append(vocabulary[phrase])
        
        # è®¡ç®—æ‰€æœ‰è¯ç»„å¯¹çš„å…±ç°
        for i, j in combinations(phrase_ids, 2):
            # ç¡®ä¿æ— å‘æ€§
            if i > j:
                i, j = j, i
            
            weight = self._calculate_edge_weight(window, i, j, vocabulary)
            cooccurrences.append((i, j, weight))
        
        return cooccurrences
    
    def _calculate_edge_weight(self, window: Window, node_i: int, node_j: int, 
                             vocabulary: Dict[str, int]) -> float:
        """
        è®¡ç®—è¾¹æƒé‡
        
        Args:
            window: å…±ç°çª—å£
            node_i: èŠ‚ç‚¹içš„ID
            node_j: èŠ‚ç‚¹jçš„ID
            vocabulary: è¯è¡¨æ˜ å°„
            
        Returns:
            float: è¾¹æƒé‡
        """
        if self.weight_method == 'binary':
            return 1.0
        elif self.weight_method == 'frequency':
            # åŸºäºè¯ç»„åœ¨çª—å£ä¸­çš„é¢‘ç‡
            reverse_vocab = {v: k for k, v in vocabulary.items()}
            phrase_i = reverse_vocab.get(node_i, '')
            phrase_j = reverse_vocab.get(node_j, '')
            
            count_i = window.phrases.count(phrase_i)
            count_j = window.phrases.count(phrase_j)
            
            return float(min(count_i, count_j))
        elif self.weight_method == 'distance':
            # åŸºäºè¯ç»„åœ¨çª—å£ä¸­çš„è·ç¦»ï¼ˆéœ€è¦ä½ç½®ä¿¡æ¯ï¼‰
            # è¿™é‡Œç®€åŒ–ä¸ºå›ºå®šæƒé‡
            return 1.0
        else:
            return 1.0


# è¾…åŠ©å‡½æ•°
def create_empty_global_graph() -> GlobalGraph:
    """
    åˆ›å»ºç©ºçš„å…¨å±€å›¾
    
    Returns:
        GlobalGraph: ç©ºçš„å…¨å±€å›¾å¯¹è±¡
    """
    return GlobalGraph(
        vocabulary={},
        reverse_vocabulary={},
        cooccurrence_matrix=scipy.sparse.csr_matrix((0, 0)),
        easygraph_instance=None,
        metadata={
            'created_at': datetime.now().isoformat(),
            'node_count': 0,
            'edge_count': 0,
            'is_empty': True
        }
    )


def merge_global_graphs(graphs: List[GlobalGraph]) -> GlobalGraph:
    """
    åˆå¹¶å¤šä¸ªå…¨å±€å›¾
    
    Args:
        graphs: è¦åˆå¹¶çš„å…¨å±€å›¾åˆ—è¡¨
        
    Returns:
        GlobalGraph: åˆå¹¶åçš„å…¨å±€å›¾
        
    Raises:
        ValueError: å½“è¾“å…¥å›¾åˆ—è¡¨ä¸ºç©ºæ—¶
    """
    if not graphs:
        raise ValueError("Cannot merge empty graph list")
    
    if len(graphs) == 1:
        return graphs[0]
    
    # åˆå¹¶è¯è¡¨
    all_phrases = set()
    for graph in graphs:
        all_phrases.update(graph.vocabulary.keys())
    
    # åˆ›å»ºæ–°çš„ç»Ÿä¸€è¯è¡¨
    vocabulary, reverse_vocabulary = create_phrase_mapping(list(all_phrases))
    
    # åˆå¹¶å…±ç°çŸ©é˜µ
    vocab_size = len(vocabulary)
    merged_matrix = scipy.sparse.csr_matrix((vocab_size, vocab_size))
    
    for graph in graphs:
        if graph.cooccurrence_matrix is not None:
            # é‡æ–°æ˜ å°„çŸ©é˜µåˆ°æ–°çš„è¯è¡¨ç©ºé—´
            old_to_new_mapping = {}
            for phrase, old_id in graph.vocabulary.items():
                new_id = vocabulary[phrase]
                old_to_new_mapping[old_id] = new_id
            
            # è¿™é‡Œéœ€è¦å®ç°çŸ©é˜µé‡æ˜ å°„é€»è¾‘
            # ç®€åŒ–å®ç°ï¼šç›´æ¥ç´¯åŠ ï¼ˆå‡è®¾è¯è¡¨ä¸€è‡´ï¼‰
            if graph.cooccurrence_matrix.shape == merged_matrix.shape:
                merged_matrix += graph.cooccurrence_matrix
    
    return GlobalGraph(
        vocabulary=vocabulary,
        reverse_vocabulary=reverse_vocabulary,
        cooccurrence_matrix=merged_matrix,
        easygraph_instance=None,  # éœ€è¦é‡æ–°æ„å»º
        metadata={
            'created_at': datetime.now().isoformat(),
            'node_count': len(vocabulary),
            'merged_from': len(graphs),
            'is_merged': True
        }
    )