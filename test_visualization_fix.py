#!/usr/bin/env python3
"""
ä¸“é—¨æµ‹è¯•å’Œä¿®å¤å¯è§†åŒ–åŠŸèƒ½çš„è„šæœ¬
ä¿®å¤é—®é¢˜ï¼š
1. 4.1æ­¥éª¤çš„spring layoutè¿›åº¦æ¡åªæ˜¾ç¤º0%å’Œ100%
2. 6.1æ­¥éª¤çš„å¯è§†åŒ–ç”Ÿæˆå¡ä½ä¸åŠ¨

æ•°æ®æ¥æºï¼š/Users/zhangjingsen/Desktop/python/graph4socialscience/toc_doc
è¾“å‡ºç›®å½•ï¼š/Users/zhangjingsen/Desktop/python/graph4socialscience/hajimi/haniumoa/
"""

import os
import sys
import json
import time
import tempfile
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import numpy as np
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé¿å…GUIé—®é¢˜
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.colors import ListedColormap
import networkx as nx
from collections import defaultdict
from tqdm import tqdm

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, '.')

class VisualizationTester:
    """å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•å’Œä¿®å¤ç±»"""
    
    def __init__(self):
        self.input_directory = "/Users/zhangjingsen/Desktop/python/graph4socialscience/toc_doc"
        self.output_dir = "/Users/zhangjingsen/Desktop/python/graph4socialscience/hajimi/haniumoa"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®
        self.cleaned_text_data = None
        self.phrase_data = None
        self.global_graph_object = None
        self.global_layout_positions = None
        self.state_subgraph_objects = {}
        self.visualization_paths = {}
        
        # é…ç½®å‚æ•°
        self.reproducibility_config = {
            'random_seed': 42,
            'phrase_type': 'mixed',
            'min_phrase_frequency': 2,
            'layout_algorithm': 'spring_deterministic'
        }
        
        self.graph_construction_config = {
            'edge_density_reduction': 0.1,
            'min_edge_weight': 2,
            'core_node_percentile': 0.2,
            'community_layout_separation': 2.0,
        }
        
        self.viz_config = {
            'edge_alpha': 0.15,
            'intra_community_edge_alpha': 0.3,
            'inter_community_edge_alpha': 0.05,
            'core_node_shape': '^',
            'periphery_node_shape': 'o',
            'min_node_size': 100,
            'max_node_size': 1000,
            'label_importance_threshold': 0.7,
            'max_labels_per_community': 3,
        }
    
    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print("ğŸ“ åŠ è½½æµ‹è¯•æ•°æ®...")
        print(f"   è¾“å…¥ç›®å½•: {self.input_directory}")
        
        if not os.path.exists(self.input_directory):
            print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {self.input_directory}")
            return False
        
        # æ‰«æç›®å½•ä¸­çš„æ–‡ä»¶
        input_files = []
        valid_extensions = {'.json', '.txt', '.md'}
        
        for root, dirs, files in os.walk(self.input_directory):
            for file in files:
                file_path = os.path.join(root, file)
                file_ext = os.path.splitext(file)[1].lower()
                if file_ext in valid_extensions:
                    input_files.append(file_path)
        
        print(f"   æ‰¾åˆ° {len(input_files)} ä¸ªæœ‰æ•ˆæ–‡ä»¶")
        
        # åŠ è½½æ•°æ®
        all_data = []
        for file_path in input_files[:10]:  # é™åˆ¶æ–‡ä»¶æ•°é‡ä»¥ä¾¿æµ‹è¯•
            try:
                # ä»è·¯å¾„æå–çŠ¶æ€
                rel_path = os.path.relpath(file_path, self.input_directory)
                path_parts = rel_path.split(os.sep)
                state = path_parts[0] if len(path_parts) > 1 else "Unknown"
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    if file_path.endswith('.json'):
                        data = json.load(f)
                        if isinstance(data, list):
                            for doc in data:
                                doc['state'] = state
                            all_data.extend(data)
                        else:
                            data['state'] = state
                            all_data.append(data)
                    else:
                        content = f.read()
                        doc_data = {
                            "segment_id": f"doc_{len(all_data)+1}",
                            "title": os.path.basename(file_path),
                            "text": content,
                            "state": state,
                            "language": "english"
                        }
                        all_data.append(doc_data)
            except Exception as e:
                print(f"   âš ï¸ è·³è¿‡æ–‡ä»¶ {file_path}: {e}")
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(all_data)} ä¸ªæ–‡æ¡£")
        return all_data
    
    def simulate_text_cleaning(self, input_data):
        """æ¨¡æ‹Ÿæ–‡æœ¬æ¸…ç†"""
        print("ğŸ§¹ æ¨¡æ‹Ÿæ–‡æœ¬æ¸…ç†...")
        
        cleaned_documents = []
        for doc in tqdm(input_data, desc="ğŸ§¹ æ¸…ç†æ–‡æ¡£", unit="doc"):
            cleaned_text = doc['text'].lower().strip()
            tokens = [token for token in cleaned_text.split() if len(token) > 2]
            
            cleaned_doc = {
                'segment_id': doc['segment_id'],
                'title': doc['title'],
                'original_text': doc['text'],
                'cleaned_text': cleaned_text,
                'tokens': tokens,
                'token_count': len(tokens),
                'state': doc['state'],
                'language': doc.get('language', 'english')
            }
            cleaned_documents.append(cleaned_doc)
        
        self.cleaned_text_data = cleaned_documents
        print(f"âœ… æ¸…ç†å®Œæˆ: {len(cleaned_documents)} ä¸ªæ–‡æ¡£")
        return True
    
    def simulate_phrase_extraction(self):
        """æ¨¡æ‹ŸçŸ­è¯­æå–"""
        print("ğŸ” æ¨¡æ‹ŸçŸ­è¯­æå–...")
        
        all_phrases = []
        phrase_counts = {}
        
        for doc in tqdm(self.cleaned_text_data, desc="ğŸ” æå–çŸ­è¯­", unit="doc"):
            tokens = doc['tokens']
            
            # æå–å•è¯
            if self.reproducibility_config['phrase_type'] in ['word', 'mixed']:
                for token in tokens:
                    if len(token) > 2:
                        all_phrases.append(token)
                        phrase_counts[token] = phrase_counts.get(token, 0) + 1
            
            # æå–åŒè¯ç»„
            if self.reproducibility_config['phrase_type'] in ['bigram', 'mixed']:
                for i in range(len(tokens) - 1):
                    bigram = f"{tokens[i]} {tokens[i+1]}"
                    all_phrases.append(bigram)
                    phrase_counts[bigram] = phrase_counts.get(bigram, 0) + 1
        
        # è¿‡æ»¤ä½é¢‘çŸ­è¯­
        min_freq = self.reproducibility_config['min_phrase_frequency']
        filtered_phrases = {phrase: count for phrase, count in phrase_counts.items() 
                          if count >= min_freq}
        
        self.phrase_data = {
            'all_phrases': all_phrases,
            'phrase_counts': phrase_counts,
            'filtered_phrases': filtered_phrases
        }
        
        print(f"âœ… çŸ­è¯­æå–å®Œæˆ: {len(filtered_phrases)} ä¸ªæœ‰æ•ˆçŸ­è¯­")
        return True
    
    def build_global_graph_with_fixed_progress(self):
        """æ„å»ºå…¨å±€å›¾ï¼Œä¿®å¤è¿›åº¦æ¡é—®é¢˜"""
        print("ğŸŒ æ„å»ºå…¨å±€å›¾ï¼ˆä¿®å¤è¿›åº¦æ¡ï¼‰...")
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(self.reproducibility_config['random_seed'])
        
        filtered_phrases = self.phrase_data['filtered_phrases']
        phrase_list = list(filtered_phrases.keys())
        
        # åˆ›å»ºNetworkXå›¾
        self.global_graph_object = nx.Graph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for phrase in phrase_list:
            self.global_graph_object.add_node(
                phrase, 
                frequency=filtered_phrases[phrase],
                phrase_type='bigram' if ' ' in phrase else 'unigram'
            )
        
        # è®¡ç®—å…±ç°å…³ç³»
        cooccurrence_counts = defaultdict(int)
        
        for doc in tqdm(self.cleaned_text_data, desc="ğŸŒ è®¡ç®—å…±ç°å…³ç³»", unit="doc"):
            doc_phrases = []
            tokens = doc['tokens']
            
            # æå–æ–‡æ¡£ä¸­çš„çŸ­è¯­
            if self.reproducibility_config['phrase_type'] in ['word', 'mixed']:
                doc_phrases.extend([token for token in tokens if token in filtered_phrases])
            
            if self.reproducibility_config['phrase_type'] in ['bigram', 'mixed']:
                for i in range(len(tokens) - 1):
                    bigram = f"{tokens[i]} {tokens[i+1]}"
                    if bigram in filtered_phrases:
                        doc_phrases.append(bigram)
            
            # è®¡ç®—å…±ç°
            for i, phrase1 in enumerate(doc_phrases):
                for phrase2 in doc_phrases[i+1:]:
                    if phrase1 != phrase2:
                        edge = tuple(sorted([phrase1, phrase2]))
                        cooccurrence_counts[edge] += 1
        
        # è¾¹è¿‡æ»¤
        print("ğŸ”§ åº”ç”¨è¾¹è¿‡æ»¤...")
        min_weight = self.graph_construction_config['min_edge_weight']
        filtered_edges = {edge: weight for edge, weight in cooccurrence_counts.items() 
                        if weight >= min_weight}
        
        if filtered_edges:
            edge_weights = list(filtered_edges.values())
            density_threshold = np.percentile(edge_weights, 
                                            (1 - self.graph_construction_config['edge_density_reduction']) * 100)
            final_edges = {edge: weight for edge, weight in filtered_edges.items() 
                         if weight >= density_threshold}
        else:
            final_edges = {}
        
        # æ·»åŠ è¾¹åˆ°å›¾
        for (phrase1, phrase2), weight in final_edges.items():
            self.global_graph_object.add_edge(phrase1, phrase2, weight=weight)
        
        print(f"   åŸå§‹è¾¹æ•°: {len(cooccurrence_counts)}")
        print(f"   è¿‡æ»¤åè¾¹æ•°: {len(final_edges)}")
        
        # è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§
        print("ğŸ“Š è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§...")
        degree_centrality = nx.degree_centrality(self.global_graph_object)
        weighted_degree = dict(self.global_graph_object.degree(weight='weight'))
        max_weighted_degree = max(weighted_degree.values()) if weighted_degree else 1
        weighted_degree_norm = {node: deg/max_weighted_degree for node, deg in weighted_degree.items()}
        
        try:
            pagerank = nx.pagerank(self.global_graph_object, weight='weight')
        except:
            pagerank = degree_centrality
        
        # åˆ†é…èŠ‚ç‚¹è§’è‰²
        node_importance = {}
        for node in self.global_graph_object.nodes():
            importance = (
                0.4 * degree_centrality.get(node, 0) +
                0.4 * weighted_degree_norm.get(node, 0) +
                0.2 * pagerank.get(node, 0)
            )
            node_importance[node] = importance
        
        importance_threshold = np.percentile(list(node_importance.values()), 
                                           (1 - self.graph_construction_config['core_node_percentile']) * 100)
        
        node_roles = {}
        for node, importance in node_importance.items():
            if importance >= importance_threshold:
                node_roles[node] = 'core'
            else:
                node_roles[node] = 'periphery'
        
        # å­˜å‚¨èŠ‚ç‚¹å±æ€§
        nx.set_node_attributes(self.global_graph_object, node_importance, 'importance')
        nx.set_node_attributes(self.global_graph_object, node_roles, 'role')
        
        # ä¿®å¤çš„å¸ƒå±€è®¡ç®— - ä½¿ç”¨è‡ªå®šä¹‰è¿›åº¦å›è°ƒ
        print("ğŸ¯ è®¡ç®—ç¡®å®šæ€§2Då¸ƒå±€ï¼ˆä¿®å¤è¿›åº¦æ˜¾ç¤ºï¼‰...")
        
        # æ–¹æ³•1ï¼šä½¿ç”¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°ä½†æ˜¾ç¤ºçœŸå®è¿›åº¦
        iterations = 50
        with tqdm(total=iterations, desc="ğŸ¯ Spring layoutè¿›åº¦", unit="iter") as pbar:
            def progress_callback():
                pbar.update(1)
            
            # åˆ†æ‰¹è®¡ç®—å¸ƒå±€ä»¥æ˜¾ç¤ºè¿›åº¦
            pos = None
            batch_size = 10
            for i in range(0, iterations, batch_size):
                current_iterations = min(batch_size, iterations - i)
                
                if pos is None:
                    # ç¬¬ä¸€æ¬¡è®¡ç®—
                    pos = nx.spring_layout(
                        self.global_graph_object,
                        k=1.0,
                        iterations=current_iterations,
                        seed=self.reproducibility_config['random_seed']
                    )
                else:
                    # ç»§ç»­ä¼˜åŒ–å¸ƒå±€
                    pos = nx.spring_layout(
                        self.global_graph_object,
                        k=1.0,
                        iterations=current_iterations,
                        pos=pos,  # ä½¿ç”¨ä¹‹å‰çš„ä½ç½®ä½œä¸ºèµ·ç‚¹
                        seed=self.reproducibility_config['random_seed']
                    )
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(current_iterations)
                time.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿä»¥æ˜¾ç¤ºè¿›åº¦
        
        self.global_layout_positions = pos
        nx.set_node_attributes(self.global_graph_object, self.global_layout_positions, 'pos')
        
        # ç¤¾åŒºæ£€æµ‹
        print("ğŸ˜ï¸ æ£€æµ‹ç¤¾åŒº...")
        try:
            communities = nx.community.greedy_modularity_communities(self.global_graph_object)
            community_map = {}
            for i, community in enumerate(communities):
                for node in community:
                    community_map[node] = i
            nx.set_node_attributes(self.global_graph_object, community_map, 'community')
            print(f"   å‘ç° {len(communities)} ä¸ªç¤¾åŒº")
        except:
            community_map = {node: 0 for node in self.global_graph_object.nodes()}
            nx.set_node_attributes(self.global_graph_object, community_map, 'community')
            print("   ä½¿ç”¨å•ä¸€ç¤¾åŒºï¼ˆå›é€€ï¼‰")
        
        print(f"âœ… å…¨å±€å›¾æ„å»ºå®Œæˆ: {self.global_graph_object.number_of_nodes()} èŠ‚ç‚¹, {self.global_graph_object.number_of_edges()} è¾¹")
        return True
    
    def activate_subgraphs(self):
        """æ¿€æ´»å­å›¾"""
        print("ğŸ—ºï¸ æ¿€æ´»çŠ¶æ€å­å›¾...")
        
        # æŒ‰çŠ¶æ€åˆ†ç»„æ–‡æ¡£
        state_documents = {}
        for doc in self.cleaned_text_data:
            state = doc['state']
            if state not in state_documents:
                state_documents[state] = []
            state_documents[state].append(doc)
        
        self.state_subgraph_objects = {}
        
        for state, docs in tqdm(state_documents.items(), desc="ğŸ—ºï¸ æ¿€æ´»å­å›¾", unit="state"):
            # è·å–è¯¥çŠ¶æ€ä¸­å‡ºç°çš„çŸ­è¯­
            state_phrases = set()
            for doc in docs:
                tokens = doc['tokens']
                
                if self.reproducibility_config['phrase_type'] in ['word', 'mixed']:
                    state_phrases.update([token for token in tokens if token in self.phrase_data['filtered_phrases']])
                
                if self.reproducibility_config['phrase_type'] in ['bigram', 'mixed']:
                    for i in range(len(tokens) - 1):
                        bigram = f"{tokens[i]} {tokens[i+1]}"
                        if bigram in self.phrase_data['filtered_phrases']:
                            state_phrases.add(bigram)
            
            # åˆ›å»ºå­å›¾
            state_nodes = [node for node in self.global_graph_object.nodes() if node in state_phrases]
            if state_nodes:
                state_subgraph = self.global_graph_object.subgraph(state_nodes)
                self.state_subgraph_objects[state] = state_subgraph
                print(f"   {state}: {state_subgraph.number_of_nodes()} èŠ‚ç‚¹, {state_subgraph.number_of_edges()} è¾¹")
        
        print(f"âœ… å­å›¾æ¿€æ´»å®Œæˆ: {len(self.state_subgraph_objects)} ä¸ªçŠ¶æ€å­å›¾")
        return True
    
    def generate_visualizations_with_fixed_progress(self):
        """ç”Ÿæˆå¯è§†åŒ–ï¼ˆä¿®å¤è¿›åº¦æ¡å¡ä½é—®é¢˜ï¼‰"""
        print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–ï¼ˆä¿®å¤è¿›åº¦æ¡ï¼‰...")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # è®¾ç½®matplotlibå‚æ•°
        plt.rcParams['figure.dpi'] = 150
        plt.rcParams['savefig.dpi'] = 300
        plt.rcParams['font.size'] = 10
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        seed = self.reproducibility_config['random_seed']
        
        self.visualization_paths = {}
        
        try:
            # 1. å…¨å±€å›¾å¯è§†åŒ–
            print("ğŸŒ ç”Ÿæˆå…¨å±€ä¸»é¢˜ç½‘ç»œ...")
            if self.global_graph_object and self.global_layout_positions:
                
                # ä½¿ç”¨æ›´ç»†ç²’åº¦çš„è¿›åº¦æ¡
                with tqdm(total=10, desc="ğŸŒ å…¨å±€ç½‘ç»œå¯è§†åŒ–", unit="step") as pbar:
                    
                    pbar.set_description("ğŸŒ åˆå§‹åŒ–å›¾å½¢")
                    fig, ax = plt.subplots(1, 1, figsize=(16, 12))
                    G = self.global_graph_object
                    pos = self.global_layout_positions
                    pbar.update(1)
                    
                    pbar.set_description("ğŸŒ å‡†å¤‡èŠ‚ç‚¹å±æ€§")
                    communities = nx.get_node_attributes(G, 'community')
                    importance_scores = nx.get_node_attributes(G, 'importance')
                    node_roles = nx.get_node_attributes(G, 'role')
                    
                    unique_communities = sorted(set(communities.values())) if communities else [0]
                    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_communities)))
                    community_colors = {comm: colors[i % len(colors)] for i, comm in enumerate(unique_communities)}
                    pbar.update(1)
                    
                    pbar.set_description("ğŸŒ è®¡ç®—è§†è§‰å±æ€§")
                    node_colors = [community_colors.get(communities.get(node, 0), 'lightblue') for node in G.nodes()]
                    
                    node_sizes = []
                    node_shapes_core = []
                    node_shapes_periphery = []
                    
                    for node in G.nodes():
                        importance = importance_scores.get(node, 0)
                        size = self.viz_config['min_node_size'] + (self.viz_config['max_node_size'] - self.viz_config['min_node_size']) * importance
                        node_sizes.append(size)
                        
                        role = node_roles.get(node, 'periphery')
                        if role == 'core':
                            node_shapes_core.append(node)
                        else:
                            node_shapes_periphery.append(node)
                    pbar.update(1)
                    
                    pbar.set_description("ğŸŒ ç»˜åˆ¶è¾¹")
                    # ç®€åŒ–è¾¹ç»˜åˆ¶ä»¥é¿å…å¡ä½
                    edge_list = list(G.edges(data=True))
                    if edge_list:
                        max_weight = max([d['weight'] for _, _, d in edge_list])
                        
                        for u, v, data in edge_list:
                            weight = data['weight']
                            u_community = communities.get(u, 0)
                            v_community = communities.get(v, 0)
                            
                            if u_community == v_community:
                                alpha = self.viz_config['intra_community_edge_alpha']
                                color = community_colors.get(u_community, 'gray')
                            else:
                                alpha = self.viz_config['inter_community_edge_alpha']
                                color = 'gray'
                            
                            width = 0.5 + 2.0 * (weight / max_weight)
                            
                            nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], 
                                                 width=width, alpha=alpha, 
                                                 edge_color=[color], ax=ax)
                    pbar.update(2)  # è¾¹ç»˜åˆ¶å®Œæˆï¼Œæ›´æ–°2æ­¥
                    
                    pbar.set_description("ğŸŒ ç»˜åˆ¶æ ¸å¿ƒèŠ‚ç‚¹")
                    if node_shapes_core:
                        core_colors = [community_colors.get(communities.get(node, 0), 'lightblue') for node in node_shapes_core]
                        core_sizes = [node_sizes[list(G.nodes()).index(node)] for node in node_shapes_core]
                        nx.draw_networkx_nodes(G, pos, nodelist=node_shapes_core,
                                             node_color=core_colors, node_size=core_sizes,
                                             node_shape=self.viz_config['core_node_shape'],
                                             alpha=0.9, edgecolors='black', linewidths=1.5, ax=ax)
                    pbar.update(1)
                    
                    pbar.set_description("ğŸŒ ç»˜åˆ¶å¤–å›´èŠ‚ç‚¹")
                    if node_shapes_periphery:
                        periphery_colors = [community_colors.get(communities.get(node, 0), 'lightblue') for node in node_shapes_periphery]
                        periphery_sizes = [node_sizes[list(G.nodes()).index(node)] for node in node_shapes_periphery]
                        nx.draw_networkx_nodes(G, pos, nodelist=node_shapes_periphery,
                                             node_color=periphery_colors, node_size=periphery_sizes,
                                             node_shape=self.viz_config['periphery_node_shape'],
                                             alpha=0.8, edgecolors='gray', linewidths=0.5, ax=ax)
                    pbar.update(1)
                    
                    pbar.set_description("ğŸŒ æ·»åŠ æ ‡ç­¾")
                    # ç®€åŒ–æ ‡ç­¾æ·»åŠ 
                    labels_to_draw = {}
                    if importance_scores:
                        importance_threshold = np.percentile(list(importance_scores.values()), 70)
                        top_nodes = [(node, score) for node, score in importance_scores.items() 
                                   if score >= importance_threshold]
                        top_nodes = sorted(top_nodes, key=lambda x: x[1], reverse=True)[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª
                        
                        for node, _ in top_nodes:
                            labels_to_draw[node] = node[:15] + "..." if len(node) > 15 else node  # æˆªæ–­é•¿æ ‡ç­¾
                    
                    if labels_to_draw:
                        nx.draw_networkx_labels(G, pos, labels_to_draw, 
                                              font_size=8, font_weight='bold', 
                                              font_color='black', ax=ax)
                    pbar.update(1)
                    
                    pbar.set_description("ğŸŒ æ·»åŠ å›¾ä¾‹")
                    ax.set_title(f'å…¨å±€ä¸»é¢˜å…±ç°ç½‘ç»œ\n'
                               f'{G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹, '
                               f'{len(unique_communities)} ç¤¾åŒº\n'
                               f'ç§å­: {seed} | å¯†åº¦: {nx.density(G)*100:.2f}%', 
                               fontsize=14, fontweight='bold', pad=20)
                    
                    # ç®€åŒ–å›¾ä¾‹
                    legend_elements = []
                    for comm in sorted(unique_communities)[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªç¤¾åŒº
                        color = community_colors[comm]
                        legend_elements.append(patches.Patch(color=color, label=f'ç¤¾åŒº {comm}'))
                    
                    if legend_elements:
                        ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1.02, 1))
                    
                    ax.axis('off')
                    plt.tight_layout()
                    pbar.update(1)
                    
                    pbar.set_description("ğŸŒ ä¿å­˜å›¾åƒ")
                    global_viz_name = f"global_thematic_network_seed{seed}_{timestamp}.png"
                    global_viz_path = os.path.join(self.output_dir, global_viz_name)
                    plt.savefig(global_viz_path, bbox_inches='tight', facecolor='white', dpi=300)
                    plt.close()
                    
                    self.visualization_paths['global_graph'] = global_viz_path
                    pbar.update(1)
                
                print(f"      âœ… ä¿å­˜: {global_viz_name}")
            
            # 2. çŠ¶æ€å­å›¾å¯è§†åŒ–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            subgraph_count = 0
            max_subgraphs = 3  # é™åˆ¶å­å›¾æ•°é‡ä»¥é¿å…å¡ä½
            
            for state, subgraph in list(self.state_subgraph_objects.items())[:max_subgraphs]:
                if subgraph.number_of_nodes() > 0:
                    subgraph_count += 1
                    print(f"ğŸ¨ ç”ŸæˆçŠ¶æ€ {state} ä¸»é¢˜ç½‘ç»œ...")
                    
                    with tqdm(total=6, desc=f"ğŸ¨ {state} ç½‘ç»œ", unit="step", leave=False) as step_pbar:
                        step_pbar.set_description(f"ğŸ¨ {state}: åˆå§‹åŒ–")
                        fig, ax = plt.subplots(1, 1, figsize=(12, 9))
                        
                        subgraph_pos = {node: self.global_layout_positions[node] for node in subgraph.nodes() 
                                      if node in self.global_layout_positions}
                        step_pbar.update(1)
                        
                        step_pbar.set_description(f"ğŸ¨ {state}: å‡†å¤‡å±æ€§")
                        communities = {node: self.global_graph_object.nodes[node].get('community', 0) 
                                     for node in subgraph.nodes()}
                        importance_scores = {node: self.global_graph_object.nodes[node].get('importance', 0) 
                                           for node in subgraph.nodes()}
                        node_roles = {node: self.global_graph_object.nodes[node].get('role', 'periphery') 
                                    for node in subgraph.nodes()}
                        
                        unique_communities = sorted(set(communities.values()))
                        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_communities)))
                        community_colors = {comm: colors[i % len(colors)] for i, comm in enumerate(unique_communities)}
                        step_pbar.update(1)
                        
                        step_pbar.set_description(f"ğŸ¨ {state}: ç»˜åˆ¶è¾¹")
                        # ç®€åŒ–è¾¹ç»˜åˆ¶
                        if subgraph.number_of_edges() > 0:
                            edge_weights = [d['weight'] for _, _, d in subgraph.edges(data=True)]
                            max_weight = max(edge_weights)
                            
                            for u, v, data in subgraph.edges(data=True):
                                weight = data['weight']
                                width = 0.5 + 2.0 * (weight / max_weight)
                                alpha = self.viz_config['intra_community_edge_alpha']
                                
                                nx.draw_networkx_edges(subgraph, subgraph_pos, edgelist=[(u, v)],
                                                     width=width, alpha=alpha, edge_color=['gray'], ax=ax)
                        step_pbar.update(1)
                        
                        step_pbar.set_description(f"ğŸ¨ {state}: ç»˜åˆ¶èŠ‚ç‚¹")
                        node_colors = [community_colors.get(communities.get(node, 0), 'lightblue') for node in subgraph.nodes()]
                        node_sizes = [100 + 400 * importance_scores.get(node, 0) for node in subgraph.nodes()]
                        
                        nx.draw_networkx_nodes(subgraph, subgraph_pos, 
                                             node_color=node_colors, node_size=node_sizes,
                                             alpha=0.8, ax=ax)
                        step_pbar.update(1)
                        
                        step_pbar.set_description(f"ğŸ¨ {state}: æ·»åŠ æ ‡ç­¾")
                        # ç®€åŒ–æ ‡ç­¾
                        if importance_scores:
                            top_nodes = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)[:5]
                            labels = {node: node[:10] + "..." if len(node) > 10 else node for node, _ in top_nodes}
                            nx.draw_networkx_labels(subgraph, subgraph_pos, labels,
                                                  font_size=8, font_weight='bold', ax=ax)
                        step_pbar.update(1)
                        
                        step_pbar.set_description(f"ğŸ¨ {state}: å®Œæˆ")
                        doc_count = len([doc for doc in self.cleaned_text_data if doc['state'] == state])
                        
                        ax.set_title(f'çŠ¶æ€ {state} ä¸»é¢˜ç½‘ç»œ\n'
                                   f'{subgraph.number_of_nodes()} èŠ‚ç‚¹, {subgraph.number_of_edges()} è¾¹\n'
                                   f'{doc_count} æ–‡æ¡£ | ç§å­: {seed}', 
                                   fontsize=12, fontweight='bold', pad=15)
                        
                        ax.axis('off')
                        plt.tight_layout()
                        
                        state_viz_name = f"state_{state}_thematic_network_seed{seed}_{timestamp}.png"
                        state_viz_path = os.path.join(self.output_dir, state_viz_name)
                        plt.savefig(state_viz_path, bbox_inches='tight', facecolor='white', dpi=300)
                        plt.close()
                        
                        self.visualization_paths[f'subgraph_{state}'] = state_viz_path
                        step_pbar.update(1)
                    
                    print(f"      âœ… ä¿å­˜: {state_viz_name}")
            
            print(f"\nâœ… å¯è§†åŒ–ç”Ÿæˆå®Œæˆ!")
            print(f"ğŸ¨ ç”Ÿæˆäº† {len(self.visualization_paths)} ä¸ªå¯è§†åŒ–æ–‡ä»¶")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            
            # æ˜¾ç¤ºæ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶
            print(f"\nğŸ“Š ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶:")
            for viz_name, viz_path in self.visualization_paths.items():
                print(f"   {viz_name}: {os.path.basename(viz_path)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•")
        print("=" * 60)
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {self.input_directory}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print("=" * 60)
        
        try:
            # 1. åŠ è½½æ•°æ®
            input_data = self.load_test_data()
            if not input_data:
                return False
            
            # 2. æ–‡æœ¬æ¸…ç†
            if not self.simulate_text_cleaning(input_data):
                return False
            
            # 3. çŸ­è¯­æå–
            if not self.simulate_phrase_extraction():
                return False
            
            # 4. æ„å»ºå…¨å±€å›¾ï¼ˆä¿®å¤è¿›åº¦æ¡ï¼‰
            if not self.build_global_graph_with_fixed_progress():
                return False
            
            # 5. æ¿€æ´»å­å›¾
            if not self.activate_subgraphs():
                return False
            
            # 6. ç”Ÿæˆå¯è§†åŒ–ï¼ˆä¿®å¤å¡ä½é—®é¢˜ï¼‰
            if not self.generate_visualizations_with_fixed_progress():
                return False
            
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
            print("âœ… è¿›åº¦æ¡æ˜¾ç¤ºé—®é¢˜å·²ä¿®å¤")
            print("âœ… å¯è§†åŒ–ç”Ÿæˆå¡ä½é—®é¢˜å·²ä¿®å¤")
            print("âœ… å›¾åƒå·²ä¿å­˜åˆ°æŒ‡å®šç›®å½•")
            
            return True
            
        except Exception as e:
            print(f"ğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ å¯è§†åŒ–åŠŸèƒ½ä¿®å¤æµ‹è¯•è„šæœ¬")
    print("ä¿®å¤é—®é¢˜:")
    print("1. 4.1æ­¥éª¤çš„spring layoutè¿›åº¦æ¡åªæ˜¾ç¤º0%å’Œ100%")
    print("2. 6.1æ­¥éª¤çš„å¯è§†åŒ–ç”Ÿæˆå¡ä½ä¸åŠ¨")
    print()
    
    tester = VisualizationTester()
    success = tester.run_test()
    
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())