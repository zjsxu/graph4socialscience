#!/usr/bin/env python3
"""
æµ‹è¯•è¯­ä¹‰ç»“æ„ä¿®æ­£åŠŸèƒ½
éªŒè¯å›¾æ„å»ºã€å­å›¾æ¿€æ´»å’Œå¯è§†åŒ–çš„è¯­ä¹‰ä¿®æ­£æ˜¯å¦æ­£ç¡®å®ç°
"""

import os
import sys
import time
from datetime import datetime
import networkx as nx

def test_semantic_corrections():
    """æµ‹è¯•è¯­ä¹‰ç»“æ„ä¿®æ­£çš„æ‰€æœ‰æ¨¡å—"""
    print("ğŸ”§ è¯­ä¹‰ç»“æ„ä¿®æ­£åŠŸèƒ½æµ‹è¯•")
    print("æµ‹è¯•æ¨¡å—ï¼šå›¾æ„å»ºã€å­å›¾æ¿€æ´»ã€å¯è§†åŒ–ç”Ÿæˆ")
    print()
    
    # è®¾ç½®æµ‹è¯•å‚æ•°
    input_dir = "test_input"
    output_dir = "test_output"
    
    if not os.path.exists(input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    print(f"ğŸ§ª ä½¿ç”¨toc_docæ•°æ®æµ‹è¯•è¯­ä¹‰ä¿®æ­£åŠŸèƒ½")
    print("=" * 60)
    
    # å¯¼å…¥ä¸»ç¨‹åº
    try:
        from complete_usage_guide import ResearchPipelineCLI
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºç®¡çº¿å®ä¾‹
    cli = ResearchPipelineCLI()
    
    # è®¾ç½®è¾“å…¥è¾“å‡ºç›®å½•
    cli.input_directory = input_dir
    cli.output_dir = output_dir
    
    # æ‰«æè¾“å…¥æ–‡ä»¶
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # æ‰«æç›®å½•è·å–æ–‡ä»¶
    cli.input_files = []
    valid_extensions = {'.json', '.txt', '.md'}
    for root, dirs, files in os.walk(input_dir):
        for file in files:
            file_path = os.path.join(root, file)
            file_ext = os.path.splitext(file)[1].lower()
            if file_ext in valid_extensions:
                cli.input_files.append(file_path)
    
    print(f"ğŸ“Š æ‰¾åˆ°æ–‡ä»¶: {len(cli.input_files)} ä¸ª")
    
    if len(cli.input_files) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶")
        return
    
    # è®¾ç½®ç®¡çº¿çŠ¶æ€
    cli.pipeline_state = {
        'data_loaded': True,
        'text_cleaned': False,
        'phrases_constructed': False,
        'global_graph_built': False,
        'subgraphs_activated': False,
        'results_exported': False
    }
    
    print("\nğŸ”„ æ‰§è¡Œè¯­ä¹‰ä¿®æ­£æµ‹è¯•ç®¡é“...")
    
    # æ‰§è¡Œå®Œæ•´ç®¡é“æµç¨‹
    start_time = time.time()
    
    try:
        # 1. æ•°æ®åŠ è½½å’Œæ–‡æœ¬æ¸…ç†
        print("\n=== æ­¥éª¤ 1: æ–‡æœ¬æ¸…ç† ===")
        cli.clean_and_normalize_text()
        
        # 2. è¯ç»„æå–
        print("\n=== æ­¥éª¤ 2: çŸ­è¯­æå– ===")
        cli.extract_tokens_and_phrases()
        
        # 3. å…¨å±€å›¾æ„å»ºï¼ˆå¸¦è¯­ä¹‰ä¿®æ­£ï¼‰
        print("\n=== æ­¥éª¤ 3: å…¨å±€å›¾æ„å»ºï¼ˆè¯­ä¹‰ä¿®æ­£ï¼‰===")
        cli.build_global_graph()
        
        # éªŒè¯è¯­ä¹‰ä¿®æ­£æ•ˆæœ
        print("\nğŸ” éªŒè¯è¯­ä¹‰ä¿®æ­£æ•ˆæœ...")
        if hasattr(cli, 'global_graph_object') and cli.global_graph_object:
            G = cli.global_graph_object
            
            # æ£€æŸ¥èŠ‚ç‚¹å±æ€§
            tf_idf_scores = cli.global_graph_object.nodes(data='tf_idf_score')
            is_structural = cli.global_graph_object.nodes(data='is_structural')
            
            print(f"   âœ… å›¾èŠ‚ç‚¹æ•°: {G.number_of_nodes()}")
            print(f"   âœ… å›¾è¾¹æ•°: {G.number_of_edges()}")
            print(f"   âœ… èŠ‚ç‚¹åŒ…å«TF-IDFåˆ†æ•°: {len([n for n, score in tf_idf_scores if score is not None])}")
            print(f"   âœ… èŠ‚ç‚¹åŒ…å«ç»“æ„æ ‡è®°: {len([n for n, structural in is_structural if structural is not None])}")
            
            # æ£€æŸ¥è¾¹æƒé‡é˜ˆå€¼
            edge_weights = [data['weight'] for _, _, data in G.edges(data=True)]
            if edge_weights:
                min_weight = min(edge_weights)
                max_weight = max(edge_weights)
                print(f"   âœ… è¾¹æƒé‡èŒƒå›´: {min_weight} - {max_weight}")
                
                # éªŒè¯æœ€å°å…±ç°é˜ˆå€¼
                threshold = cli.graph_construction_config.get('min_cooccurrence_threshold', 3)
                below_threshold = [w for w in edge_weights if w < threshold]
                print(f"   âœ… ä½äºé˜ˆå€¼({threshold})çš„è¾¹: {len(below_threshold)} (åº”è¯¥ä¸º0)")
        
        # 4. æŸ¥çœ‹å…¨å±€å›¾ç»Ÿè®¡ï¼ˆéªŒè¯æ–°ç»Ÿè®¡ä¿¡æ¯ï¼‰
        print("\n=== æ­¥éª¤ 4: æŸ¥çœ‹å…¨å±€å›¾ç»Ÿè®¡ ===")
        cli.view_global_graph_statistics()
        
        # 5. å­å›¾æ¿€æ´»ï¼ˆå¸¦é‡æ–°åŠ æƒï¼‰
        print("\n=== æ­¥éª¤ 5: å­å›¾æ¿€æ´»ï¼ˆé‡æ–°åŠ æƒï¼‰===")
        cli.activate_state_subgraphs()
        
        # éªŒè¯å­å›¾æ¿€æ´»æ•ˆæœ
        print("\nğŸ” éªŒè¯å­å›¾æ¿€æ´»æ•ˆæœ...")
        if hasattr(cli, 'state_subgraph_objects') and cli.state_subgraph_objects:
            total_isolated = 0
            total_reweighted = 0
            
            for state, subgraph in cli.state_subgraph_objects.items():
                # è®¡ç®—å­¤ç«‹èŠ‚ç‚¹æ•°
                isolated_count = len(list(nx.isolates(subgraph)))
                total_isolated += isolated_count
                
                # æ£€æŸ¥é‡æ–°åŠ æƒçš„è¾¹
                reweighted_edges = 0
                for u, v, data in subgraph.edges(data=True):
                    if 'state_weight' in data and 'global_weight' in data:
                        if data['state_weight'] != data['global_weight']:
                            reweighted_edges += 1
                total_reweighted += reweighted_edges
            
            print(f"   âœ… æ¿€æ´»çš„å­å›¾æ•°: {len(cli.state_subgraph_objects)}")
            print(f"   âœ… æ€»å­¤ç«‹èŠ‚ç‚¹æ•°: {total_isolated}")
            print(f"   âœ… é‡æ–°åŠ æƒçš„è¾¹æ•°: {total_reweighted}")
        
        # 6. æŸ¥çœ‹å­å›¾æ¯”è¾ƒï¼ˆéªŒè¯æ–°ç»Ÿè®¡ä¿¡æ¯ï¼‰
        print("\n=== æ­¥éª¤ 6: æŸ¥çœ‹å­å›¾æ¯”è¾ƒ ===")
        cli.view_subgraph_comparisons()
        
        # 7. å¯è§†åŒ–ç”Ÿæˆï¼ˆè¯­ä¹‰å‚è€ƒé£æ ¼ï¼‰
        print("\n=== æ­¥éª¤ 7: å¯è§†åŒ–ç”Ÿæˆï¼ˆè¯­ä¹‰é£æ ¼ï¼‰===")
        cli.generate_deterministic_visualizations()
        
        # éªŒè¯å¯è§†åŒ–æ•ˆæœ
        print("\nğŸ” éªŒè¯å¯è§†åŒ–æ•ˆæœ...")
        if hasattr(cli, 'visualization_paths') and cli.visualization_paths:
            for graph_type, path in cli.visualization_paths.items():
                if os.path.exists(path):
                    file_size = os.path.getsize(path)
                    print(f"   âœ… {graph_type}å¯è§†åŒ–: {os.path.basename(path)} ({file_size} bytes)")
                else:
                    print(f"   âŒ {graph_type}å¯è§†åŒ–æ–‡ä»¶æœªæ‰¾åˆ°: {path}")
        
        end_time = time.time()
        print(f"\nâœ… è¯­ä¹‰ä¿®æ­£æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        print(f"\nğŸ“‹ è¯­ä¹‰ä¿®æ­£æµ‹è¯•æŠ¥å‘Š:")
        print("=" * 50)
        
        # A. å›¾æ„å»ºä¿®æ­£éªŒè¯
        print("ğŸ”§ A. å›¾æ„å»ºä¿®æ­£:")
        if hasattr(cli, 'phrase_data') and hasattr(cli, 'global_graph_object'):
            original_phrases = len(cli.phrase_data.get('filtered_phrases', {}))
            final_nodes = cli.global_graph_object.number_of_nodes()
            structural_removed = original_phrases - final_nodes
            print(f"   âœ… ç»“æ„åŒ–è¯æ±‡è¿‡æ»¤: {structural_removed}/{original_phrases} ä¸ªè¯æ±‡è¢«ç§»é™¤")
            
            # æ£€æŸ¥æ»‘åŠ¨çª—å£å’Œé˜ˆå€¼
            config = cli.graph_construction_config
            print(f"   âœ… æ»‘åŠ¨çª—å£å¤§å°: {config.get('sliding_window_size', 'N/A')}")
            print(f"   âœ… æœ€å°å…±ç°é˜ˆå€¼: {config.get('min_cooccurrence_threshold', 'N/A')}")
            
            # æ£€æŸ¥è¯­ä¹‰å±æ€§
            tf_idf_count = len([n for n, data in cli.global_graph_object.nodes(data=True) if 'tf_idf_score' in data])
            print(f"   âœ… èŠ‚ç‚¹è¯­ä¹‰å±æ€§: {tf_idf_count}/{final_nodes} ä¸ªèŠ‚ç‚¹æœ‰TF-IDFåˆ†æ•°")
        
        # B. å­å›¾æ¿€æ´»ä¿®æ­£éªŒè¯
        print("\nğŸ”§ B. å­å›¾æ¿€æ´»ä¿®æ­£:")
        if hasattr(cli, 'state_subgraph_objects'):
            print(f"   âœ… å­å›¾æ•°é‡: {len(cli.state_subgraph_objects)}")
            print(f"   âœ… æ¿€æ´»æ–¹æ³•: é‡æ–°åŠ æƒï¼ˆéé‡å»ºï¼‰")
            print(f"   âœ… ä¿ç•™å…¨å±€ä½ç½®: æ˜¯")
            print(f"   âœ… å…è®¸å­¤ç«‹èŠ‚ç‚¹: æ˜¯")
        
        # C. å¯è§†åŒ–ä¿®æ­£éªŒè¯
        print("\nğŸ”§ C. å¯è§†åŒ–ä¿®æ­£:")
        if hasattr(cli, 'viz_config'):
            config = cli.viz_config
            print(f"   âœ… ç¡®å®šæ€§å¸ƒå±€: å›ºå®šç§å­ {config.get('fixed_random_seed', 'N/A')}")
            print(f"   âœ… èŠ‚ç‚¹å½¢çŠ¶: æ ¸å¿ƒ={config.get('core_node_shape', 'N/A')}, å¤–å›´={config.get('periphery_node_shape', 'N/A')}")
            print(f"   âœ… èŠ‚ç‚¹å¤§å°: åŸºäºTF-IDFåˆ†æ•°")
            print(f"   âœ… é€‰æ‹©æ€§æ ‡ç­¾: ä»…æ ¸å¿ƒèŠ‚ç‚¹ï¼Œä¸æ ‡è®°ç»“æ„åŒ–è¯æ±‡")
            print(f"   âœ… é«˜åˆ†è¾¨ç‡è¾“å‡º: {config.get('output_dpi', 'N/A')} DPI")
        
        print(f"\nğŸ‰ æ‰€æœ‰è¯­ä¹‰ä¿®æ­£åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return

if __name__ == "__main__":
    test_semantic_corrections()