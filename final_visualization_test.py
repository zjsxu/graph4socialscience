#!/usr/bin/env python3
"""
æœ€ç»ˆå¯è§†åŒ–ä¿®å¤éªŒè¯æµ‹è¯•
éªŒè¯complete_usage_guide.pyä¸­çš„6.1æ“ä½œæ˜¯å¦å½»åº•ä¿®å¤
"""

import os
import sys
import json
import tempfile
import shutil
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, '.')

def create_realistic_test_data():
    """åˆ›å»ºæ›´çœŸå®çš„æµ‹è¯•æ•°æ®ï¼Œæ¨¡æ‹Ÿç”¨æˆ·çš„å®é™…ä½¿ç”¨åœºæ™¯"""
    test_dir = tempfile.mkdtemp(prefix="final_viz_test_")
    
    # åˆ›å»ºæ›´å¤æ‚çš„æµ‹è¯•æ–‡æ¡£ï¼Œæ¨¡æ‹ŸçœŸå®çš„TOCæ•°æ®
    test_docs = [
        {
            "segment_id": "ai_research_001",
            "title": "Artificial Intelligence Fundamentals",
            "text": "artificial intelligence machine learning deep learning neural networks computer vision natural language processing pattern recognition supervised learning unsupervised learning reinforcement learning feature engineering model evaluation cross validation hyperparameter tuning ensemble methods random forest support vector machines logistic regression linear regression decision trees clustering algorithms dimensionality reduction principal component analysis",
            "state": "CA",
            "language": "english"
        },
        {
            "segment_id": "ai_research_002", 
            "title": "Deep Learning Applications",
            "text": "deep learning neural networks convolutional neural networks recurrent neural networks transformer models attention mechanisms computer vision image recognition object detection natural language processing machine translation text classification sentiment analysis language modeling generative adversarial networks variational autoencoders transfer learning fine tuning pre trained models",
            "state": "CA",
            "language": "english"
        },
        {
            "segment_id": "data_science_001",
            "title": "Data Science Methodology",
            "text": "data science analytics big data machine learning statistical modeling predictive analytics business intelligence data visualization data mining exploratory data analysis feature selection model validation statistical inference hypothesis testing regression analysis time series analysis clustering classification anomaly detection",
            "state": "NY",
            "language": "english"
        },
        {
            "segment_id": "data_science_002",
            "title": "Big Data Technologies", 
            "text": "big data cloud computing distributed systems scalable architectures apache spark hadoop mapreduce data warehousing data lakes etl processes real time processing stream processing batch processing data pipelines data governance data quality data integration data transformation",
            "state": "NY",
            "language": "english"
        },
        {
            "segment_id": "ml_algorithms_001",
            "title": "Machine Learning Algorithms",
            "text": "machine learning algorithms supervised learning unsupervised learning semi supervised learning reinforcement learning deep reinforcement learning multi agent systems neural architecture search automated machine learning hyperparameter optimization bayesian optimization genetic algorithms evolutionary computation swarm intelligence",
            "state": "TX",
            "language": "english"
        },
        {
            "segment_id": "ml_algorithms_002",
            "title": "Advanced ML Techniques",
            "text": "advanced machine learning techniques ensemble learning boosting bagging stacking meta learning few shot learning zero shot learning multi task learning transfer learning domain adaptation adversarial training robust optimization federated learning privacy preserving machine learning differential privacy homomorphic encryption",
            "state": "TX",
            "language": "english"
        },
        {
            "segment_id": "ai_ethics_001",
            "title": "AI Ethics and Fairness",
            "text": "artificial intelligence ethics algorithmic fairness bias detection bias mitigation explainable artificial intelligence interpretable machine learning model transparency accountability responsible artificial intelligence ethical artificial intelligence fairness metrics demographic parity equalized odds calibration",
            "state": "FL",
            "language": "english"
        },
        {
            "segment_id": "ai_ethics_002",
            "title": "AI Safety and Governance",
            "text": "artificial intelligence safety ai governance ai regulation ai policy algorithmic accountability transparency explainability interpretability robustness adversarial attacks adversarial examples model security privacy protection data protection gdpr compliance ethical guidelines ai standards",
            "state": "FL",
            "language": "english"
        }
    ]
    
    # ä¿å­˜æµ‹è¯•æ–‡æ¡£åˆ°çŠ¶æ€æ–‡ä»¶å¤¹
    for doc in test_docs:
        state_dir = os.path.join(test_dir, doc['state'])
        os.makedirs(state_dir, exist_ok=True)
        
        file_path = os.path.join(state_dir, f"{doc['segment_id']}.json")
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(doc, f, indent=2, ensure_ascii=False)
    
    return test_dir

def test_complete_pipeline_with_visualization():
    """æµ‹è¯•å®Œæ•´ç®¡é“ï¼Œé‡ç‚¹éªŒè¯6.1æ“ä½œ"""
    print("ğŸ§ª æœ€ç»ˆå¯è§†åŒ–ä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_dir = create_realistic_test_data()
    output_dir = "test_output"
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # å¯¼å…¥ä¿®å¤åçš„ç±»
        from complete_usage_guide import ResearchPipelineCLI
        
        # åˆå§‹åŒ–åº”ç”¨
        app = ResearchPipelineCLI()
        app.input_directory = test_dir
        app.output_dir = output_dir
        
        # æ‰«æè¾“å…¥æ–‡ä»¶
        app.input_files = []
        for root, dirs, files in os.walk(test_dir):
            for file in files:
                if file.endswith('.json'):
                    app.input_files.append(os.path.join(root, file))
        
        app.pipeline_state['data_loaded'] = True
        
        print(f"ğŸ“ æµ‹è¯•æ•°æ®: {test_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ“Š æµ‹è¯•æ–‡ä»¶: {len(app.input_files)} ä¸ª")
        print(f"ğŸ“Š æµ‹è¯•çŠ¶æ€: {len(set(doc['state'] for doc in create_realistic_test_data() if 'state' in str(doc)))} ä¸ª")
        
        # æ‰§è¡Œå®Œæ•´ç®¡é“
        print("\nğŸ”„ æ‰§è¡Œå®Œæ•´ç®¡é“...")
        
        # æ­¥éª¤2.1: æ–‡æœ¬æ¸…ç†
        print("\n2.1 æ–‡æœ¬æ¸…ç†...")
        app.clean_and_normalize_text()
        if not app.pipeline_state['text_cleaned']:
            print("âŒ æ–‡æœ¬æ¸…ç†å¤±è´¥")
            return False
        
        # æ­¥éª¤3.2: çŸ­è¯­æå–
        print("\n3.2 çŸ­è¯­æå–...")
        app.extract_tokens_and_phrases()
        if not app.pipeline_state['phrases_constructed']:
            print("âŒ çŸ­è¯­æå–å¤±è´¥")
            return False
        
        # æ­¥éª¤4.1: å…¨å±€å›¾æ„å»º
        print("\n4.1 å…¨å±€å›¾æ„å»º...")
        app.build_global_graph()
        if not app.pipeline_state['global_graph_built']:
            print("âŒ å…¨å±€å›¾æ„å»ºå¤±è´¥")
            return False
        
        # æ­¥éª¤5.1: å­å›¾æ¿€æ´»
        print("\n5.1 å­å›¾æ¿€æ´»...")
        app.activate_state_subgraphs()
        if not app.pipeline_state['subgraphs_activated']:
            print("âŒ å­å›¾æ¿€æ´»å¤±è´¥")
            return False
        
        # å…³é”®æµ‹è¯•ï¼šæ­¥éª¤6.1 å¯è§†åŒ–ç”Ÿæˆ
        print("\nğŸ¯ å…³é”®æµ‹è¯•ï¼š6.1 å¯è§†åŒ–ç”Ÿæˆï¼ˆä¿®å¤éªŒè¯ï¼‰...")
        print("   è¿™æ˜¯ä¹‹å‰å¡ä½çš„æ­¥éª¤ï¼Œç°åœ¨æµ‹è¯•æ˜¯å¦ä¿®å¤")
        
        start_time = datetime.now()
        app.generate_deterministic_visualizations()
        end_time = datetime.now()
        
        duration = (end_time - start_time).total_seconds()
        print(f"   â±ï¸ å¯è§†åŒ–ç”Ÿæˆè€—æ—¶: {duration:.2f}ç§’")
        
        # éªŒè¯ç»“æœ
        if not hasattr(app, 'visualization_paths') or not app.visualization_paths:
            print("âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥ï¼šæ²¡æœ‰ç”Ÿæˆå¯è§†åŒ–æ–‡ä»¶")
            return False
        
        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
        generated_files = []
        missing_files = []
        
        for viz_name, viz_path in app.visualization_paths.items():
            if os.path.exists(viz_path):
                file_size = os.path.getsize(viz_path)
                generated_files.append((viz_name, os.path.basename(viz_path), file_size))
                print(f"   âœ… {viz_name}: {os.path.basename(viz_path)} ({file_size} bytes)")
            else:
                missing_files.append((viz_name, viz_path))
                print(f"   âŒ {viz_name}: æ–‡ä»¶ä¸å­˜åœ¨ - {viz_path}")
        
        if missing_files:
            print(f"âŒ æœ‰ {len(missing_files)} ä¸ªæ–‡ä»¶ç¼ºå¤±")
            return False
        
        # æˆåŠŸéªŒè¯
        print(f"\nğŸ‰ æœ€ç»ˆéªŒè¯æˆåŠŸ!")
        print(f"âœ… 6.1æ“ä½œä¸å†å¡ä½ï¼ŒæˆåŠŸç”Ÿæˆ {len(generated_files)} ä¸ªå¯è§†åŒ–æ–‡ä»¶")
        print(f"âœ… æ€»è€—æ—¶: {duration:.2f}ç§’ï¼ˆåˆç†èŒƒå›´å†…ï¼‰")
        print(f"âœ… æ‰€æœ‰æ–‡ä»¶éƒ½å·²æ­£ç¡®ç”Ÿæˆåˆ°ç›®æ ‡ç›®å½•")
        
        # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        if hasattr(app, 'global_graph_object') and app.global_graph_object:
            G = app.global_graph_object
            print(f"\nğŸ“Š å¤„ç†çš„å›¾è§„æ¨¡:")
            print(f"   èŠ‚ç‚¹æ•°: {G.number_of_nodes()}")
            print(f"   è¾¹æ•°: {G.number_of_edges()}")
            print(f"   å¯†åº¦: {nx.density(G)*100:.2f}%")
            print(f"   çŠ¶æ€å­å›¾æ•°: {len(app.state_subgraph_objects)}")
        
        return True
        
    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        try:
            shutil.rmtree(test_dir)
            print(f"\nğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®: {test_dir}")
        except:
            pass

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æœ€ç»ˆå¯è§†åŒ–ä¿®å¤éªŒè¯")
    print("éªŒè¯6.1æ“ä½œçš„æ€§èƒ½é—®é¢˜æ˜¯å¦å½»åº•è§£å†³")
    print()
    
    success = test_complete_pipeline_with_visualization()
    
    if success:
        print("\nğŸŠ ä¿®å¤éªŒè¯å®Œå…¨æˆåŠŸ!")
        print("âœ… 6.1æ“ä½œçš„å¡ä½é—®é¢˜å·²å½»åº•è§£å†³")
        print("âœ… æ€§èƒ½é—®é¢˜å·²ä¿®å¤ï¼ˆé¢„è®¡ç®—max_weightï¼‰")
        print("âœ… å¯è§†åŒ–ç”Ÿæˆé€Ÿåº¦æ­£å¸¸")
        print("âœ… æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        print()
        print("ğŸ“‹ ç°åœ¨å¯ä»¥å®‰å…¨ä½¿ç”¨complete_usage_guide.pyå¤„ç†çœŸå®æ•°æ®:")
        print("   python complete_usage_guide.py")
        print("   é€‰æ‹©6.1æ“ä½œä¸ä¼šå†å¡ä½")
    else:
        print("\nâŒ ä¿®å¤éªŒè¯å¤±è´¥")
        print("éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•6.1æ“ä½œ")
    
    return 0 if success else 1

if __name__ == "__main__":
    import networkx as nx  # éœ€è¦å¯¼å…¥nxç”¨äºå¯†åº¦è®¡ç®—
    sys.exit(main())