#!/usr/bin/env python3
"""
æ‰¹å¤„ç†åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•æ‰¹å¤„ç†å’Œè¾“å‡ºç®¡ç†ç³»ç»Ÿçš„åŸºæœ¬åŠŸèƒ½ã€‚
"""

import os
import json
import tempfile
import shutil
from pathlib import Path

from semantic_coword_pipeline import SemanticCowordPipeline, Config, TOCDocument


def create_test_data(test_dir: str) -> None:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    test_path = Path(test_dir)
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_docs = [
        {
            "segment_id": "doc1_seg1",
            "title": "Introduction to Machine Learning",
            "level": 1,
            "order": 1,
            "text": "Machine learning is a subset of artificial intelligence. It involves algorithms that learn from data.",
            "state": "CA"
        },
        {
            "segment_id": "doc1_seg2", 
            "title": "Deep Learning Basics",
            "level": 2,
            "order": 2,
            "text": "Deep learning uses neural networks with multiple layers. These networks can learn complex patterns.",
            "state": "CA"
        },
        {
            "segment_id": "doc2_seg1",
            "title": "Natural Language Processing",
            "level": 1,
            "order": 1,
            "text": "Natural language processing enables computers to understand human language. It combines linguistics and machine learning.",
            "state": "NY"
        }
    ]
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    for i, doc in enumerate(test_docs):
        file_path = test_path / f"test_doc_{i+1}.json"
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(doc, f, indent=2)
    
    print(f"Created {len(test_docs)} test documents in {test_dir}")


def test_batch_processing():
    """æµ‹è¯•æ‰¹å¤„ç†åŠŸèƒ½"""
    print("Testing batch processing functionality...")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        input_dir = Path(temp_dir) / "input"
        output_dir = Path(temp_dir) / "output"
        
        input_dir.mkdir()
        output_dir.mkdir()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        create_test_data(str(input_dir))
        
        try:
            # åˆ›å»ºç®¡çº¿å®ä¾‹
            pipeline = SemanticCowordPipeline()
            
            # è¿è¡Œæ‰¹å¤„ç†
            result = pipeline.run(str(input_dir), str(output_dir))
            
            # éªŒè¯ç»“æœ
            print(f"âœ“ Processed {result.processed_files}/{result.total_files} files")
            print(f"âœ“ Processing time: {result.processing_time:.2f} seconds")
            print(f"âœ“ Generated {len(result.output_files)} output files")
            
            if result.global_graph:
                print(f"âœ“ Global graph has {result.global_graph.get_node_count()} nodes")
            
            print(f"âœ“ Generated {len(result.state_subgraphs)} state subgraphs")
            
            # éªŒè¯è¾“å‡ºæ–‡ä»¶å­˜åœ¨
            output_files_exist = all(Path(f).exists() for f in result.output_files)
            if output_files_exist:
                print("âœ“ All output files were created successfully")
            else:
                print("âœ— Some output files are missing")
            
            # æ£€æŸ¥è¾“å‡ºç›®å½•ç»“æ„
            expected_dirs = ['data', 'graphs', 'reports', 'logs']
            for dir_name in expected_dirs:
                dir_path = output_dir / dir_name
                if dir_path.exists():
                    print(f"âœ“ Output directory '{dir_name}' created")
                else:
                    print(f"âœ— Output directory '{dir_name}' missing")
            
            print("\nâœ“ Batch processing test completed successfully!")
            return True
            
        except Exception as e:
            print(f"âœ— Batch processing test failed: {e}")
            return False


def test_configuration():
    """æµ‹è¯•é…ç½®åŠŸèƒ½"""
    print("\nTesting configuration functionality...")
    
    try:
        # åˆ›å»ºé…ç½®å®ä¾‹
        config = Config()
        
        # æµ‹è¯•é…ç½®è·å–
        batch_size = config.get('performance.batch_size', 1000)
        print(f"âœ“ Retrieved batch size: {batch_size}")
        
        # æµ‹è¯•é…ç½®è®¾ç½®
        config.set('performance.batch_size', 500)
        new_batch_size = config.get('performance.batch_size')
        assert new_batch_size == 500
        print("âœ“ Configuration setting works")
        
        # æµ‹è¯•é…ç½®éªŒè¯
        validation_result = config.validate()
        if not validation_result['errors']:
            print("âœ“ Configuration validation passed")
        else:
            print(f"âœ— Configuration validation errors: {validation_result['errors']}")
        
        print("âœ“ Configuration test completed successfully!")
        return True
        
    except Exception as e:
        print(f"âœ— Configuration test failed: {e}")
        return False


def test_data_models():
    """æµ‹è¯•æ•°æ®æ¨¡å‹"""
    print("\nTesting data models...")
    
    try:
        # æµ‹è¯•TOCDocument
        doc_data = {
            "segment_id": "test_seg_1",
            "title": "Test Document",
            "level": 1,
            "order": 1,
            "text": "This is a test document for validation.",
            "state": "TEST"
        }
        
        toc_doc = TOCDocument.from_json(doc_data)
        assert toc_doc.segment_id == "test_seg_1"
        assert toc_doc.state == "TEST"
        print("âœ“ TOCDocument creation and validation works")
        
        # æµ‹è¯•è½¬æ¢ä¸ºå­—å…¸
        doc_dict = toc_doc.to_dict()
        assert doc_dict['segment_id'] == "test_seg_1"
        print("âœ“ TOCDocument to_dict conversion works")
        
        print("âœ“ Data models test completed successfully!")
        return True
        
    except Exception as e:
        print(f"âœ— Data models test failed: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("Semantic Coword Pipeline - Batch Processing Test")
    print("=" * 60)
    
    tests = [
        ("Configuration", test_configuration),
        ("Data Models", test_data_models),
        ("Batch Processing", test_batch_processing)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'-' * 40}")
        print(f"Running {test_name} Test")
        print(f"{'-' * 40}")
        
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"âœ— {test_name} test failed with exception: {e}")
            results.append((test_name, False))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
    print(f"\n{'=' * 60}")
    print("Test Results Summary")
    print(f"{'=' * 60}")
    
    passed = 0
    for test_name, success in results:
        status = "PASSED" if success else "FAILED"
        print(f"{test_name:20} : {status}")
        if success:
            passed += 1
    
    print(f"\nOverall: {passed}/{len(results)} tests passed")
    
    if passed == len(results):
        print("ğŸ‰ All tests passed! Batch processing system is working correctly.")
        return 0
    else:
        print("âŒ Some tests failed. Please check the implementation.")
        return 1


if __name__ == "__main__":
    exit(main())